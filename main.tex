\documentclass[10pt]{article}

% nesterov -> Nesterov
% donut -> doughnuts

% add links to document
\PassOptionsToPackage{hyphens}{url}\usepackage[hidelinks]{hyperref}

% add some packages
\usepackage{xcolor}
\usepackage[english]{babel}
\usepackage{nameref}
\usepackage{footnote}
\usepackage{refcount}
\usepackage{makecell}
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{titlesec}
\usepackage{graphicx,import}
\usepackage{setspace}
\usepackage{url}
\usepackage{amsmath,amsfonts,amssymb}
%\usepackage{indentfirst}
\usepackage{float}
\usepackage{pgf}
\usepackage{pdfpages}
\usepackage{svg}
\usepackage{pstricks}
\usepackage{color,soul}
\usepackage{pst-plot}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[style=verbose-ibid,backend=bibtex]{biblatex}
\usepackage{multirow}
\usepackage{caption}
\usepackage{colortbl}\usepackage{float}
\usepackage{placeins}
\usepackage{dblfloatfix}
\usepackage{tikz}
\usepackage{environ}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{verbdef}

% tikzpiture libraries
\usetikzlibrary{positioning}
\usetikzlibrary{math}
\usetikzlibrary{backgrounds}
\usetikzlibrary{calc}

% circled command
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
   \node[shape=circle,draw,inner sep=1pt] (char) {#1};}}

\restylefloat{table}

% add more distance between footnotes and the main text
\addtolength{\skip\footins}{2pc plus 5pt}

% italic blockquotes
\renewcommand{\mkbegdispquote}[2]{\itshape}
\renewcommand{\mkbegdispquote}[2]{\openautoquote}
\renewcommand{\mkenddispquote}[2]{\closeautoquote#1#2}

% fullref command
%\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1}: \nameref*{#1}}}

% table border commands
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\newcommand{\DrawBox}[3][]{%
    \tikz[overlay,remember picture]{
    \draw[black,#1]
      ($(#2) + (-0.85em,  2.40ex)$) rectangle
      ($(#3) + ( 0.85em, -1.30ex)$);}
}

% literature reference
\bibliography{references}

% document settings
\author{Björn Hempel <bjoern@hempel.li>}

% add nice formated tables
\makesavenoteenv{tabular}
\makesavenoteenv{table}

% add automatic numbering
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{4}
\setlength\bibitemsep{1.5\itemsep}

% some formating settings
\setlength{\parindent}{20pt}
\setlength{\parskip}{5pt}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% Footmarks always at the end of the page
\usepackage[bottom]{footmisc}

% allow some special characters
\DeclareUnicodeCharacter{2212}{-}

% pgf figures with images (add function \inputpgf)
\newcommand\inputpgf[2]{{
\let\pgfimageWithoutPath\pgfimage
\renewcommand{\pgfimage}[2][]{\pgfimageWithoutPath[##1]{#1/##2}}
\input{#1/#2}
}}

% import function of tex files
\newcommand{\includetex}[1]{
    \def\svgwidth{\columnwidth}
    \import{images/other/}{#1.pdf_tex}
}

% add language switcher functions \de and \en
\newif\ifen
\newif\ifde
\newcommand{\en}[1]{\ifen#1\fi}
\newcommand{\de}[1]{\ifde#1\fi}

% language switcher
%\detrue
\entrue



% -------------------- %
% Start document %
% -------------------- %
\begin{document}



	% -------------------- %
	% Flyleaf %
	% -------------------- %
	\input{pages/0_1_flyleaf}
	\pagebreak



	% -------------------- %
	% Empty page %
	% -------------------- %
    \newpage\null\thispagestyle{empty}\newpage



	% -------------------- %
	% Abstract %
	% -------------------- %
	\section*{Abstract}
		\de{Neuronale Netze sind außerordentlich gut darin Muster in Daten zu finden. Hierzu müssen diese Netze (Modelle) vorher mit bekannten Datensätzen trainiert und entsprechend angepasst werden. Datensätze sind meist sehr teuer in der Beschaffung und sollten deshalb mit Bedacht und guter Qualität eingesetzt werden. Das Training der Modelle findet unter dem Einfluss vieler Hyperparametern statt, welche es vorher zu bestimmen gilt. Dabei ist darauf zu achten das bestmögliche Modell mit seinen bestmöglichen Parametern zu verwenden. In dieser Arbeit wird auf das Erstellen von Modellen für die Bildklassifikation mittels maschinellem Lernen eingegangen und es werden die Modellgenauigkeiten miteinander verglichen. Das Hauptziel der Arbeit ist es, optimale Parameter und Techniken für die Klassifikation zu finden, die es auch ermöglichen, mit wenig Trainingsdaten ein optimales Modell zu erstellen.}
		\en{Neural networks are extraordinarily good at finding patterns in data. Therefore these networks (models) have to be trained with known data sets and adapted accordingly. Data sets are usually very expensive to obtain and should therefore be used with care and good quality. The training of the models takes place under the influence of many hyperparameters, which have to be determined beforehand. It is important to use the best possible model with its best possible parameters. In this thesis the creation of models for image classification by machine learning is discussed and the model accuracies are compared. The main goal of the thesis is to find optimal parameters and techniques for classification, which also allows to create an optimal model with little training data.}
	\pagebreak



	% -------------------- %
	% Table of contents %
	% -------------------- %
	\tableofcontents
	\pagebreak


	% -------------------- %
	% Introduction %
	% -------------------- %
	\section{Introduction}
		\de{In dieser Arbeit wird auf der Erstellung von Modellen zur Bildklassifikation mittels maschinellem Lernen eingegangen. Viele variable Parameter (Hyperparameter) werden beim Erstellen der Modelle einen entscheidenden Einfluss auf die Genauigkeit des Modells haben und werden hier im Detail verglichen. Nicht immer ist nur die Genauigkeit ein ausschlaggebender Faktor. Auch die benötigte Rechenzeit, welche notwendig ist das Modell zu bestimmen, soll nicht außer Acht gelassen werden und mit in die Auswertung einbezogen werden. Ich gehe davon aus, dass eine kleine Lernrate verbunden mit vielen Lern-Epochen und entsprechend mehr benötigter Rechenzeit bessere Ergebnisse erzielen werden, als wenige Lernepochen verbunden mit einer hohe Lernrate (langsame Anpassung vs. schnelle Anpassung). Auch gehe ich davon aus, dass eine hohe Qualität und eine größere Menge an Daten das Ergebnis entschieden positiv beeinflussen werden. Neue und komplexere Convolutional Neuronale Netzwerke stufe ich erfolgreicher in der Modellgenauigkeit ein als schön etwas ältere und kleinere Modelle.}
		\en{This thesis deals with the creation of models for image classification using machine learning. Many variable parameters (hyperparameters) will have a decisive influence on the accuracy of the model when creating the models and are compared here in detail. Not always only the accuracy is a decisive factor. Also, the required computing time, which is necessary to determine the model, should not be disregarded and should be included in the evaluation. I assume that a small learning rate combined with many learning epochs and correspondingly more computing time required will achieve better results than a few learning epochs combined with a high learning rate (slow adaptation vs. fast adaptation). I also assume that a high quality and a larger amount of data will have a decidedly positive influence on the result. New and more complex convolutional neural networks are more successful in model accuracy than older and smaller models.}


	% -------------------- %
	% Background %
	% -------------------- %
	\section{Background}

		% -------------------- %
		% Image Classification %
		% -------------------- %
		\subsection{Image classification}
			\de{Klassifizierungen sind ein Prozess der Identifizierung, zu welcher Klasse ein unbeobachtetes Objekt gehört. Hierbei können eine Reihe von vordefinierten Klassen vorgegeben und anhand deren Eigenschaften versucht werden unbekannte und bisher unbeobachtete Objekt einzuordnen. Bei der Bildklassifizierung wird analog vorgegangen. Bei den zuvor genannten Objekten handelt es sich nun schlichtweg um Bilder.}
			\en{Classifications are a process of identifying to which class an unobserved object belongs. Several predefined classes can be specified and, based on their properties, an attempt can be made to classify unknown and previously unobserved objects. The procedure for image classification is similar. The previously mentioned objects are now simply images.}

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.4\textwidth]{images/cat_dog}
				\caption[Is it a dog or a cat?]{Is it a dog or a cat?\footnotemark}
				\label{fig:cat_or_dog}
			\end{figure}
			\footnotetext{Source: \url{https://towardsdatascience.com/image-classifier-cats-vs-dogs-with-convolutional-neural-networks-cnns-and-google-colabs-4e9af21ae7a8}}

			\de{Lange Zeit galt die automatische Erkennung von Objekten, Personen und Szenen in Bildern durch Computer als unmöglich. Die Komplexität schien zu groß, als dass man sie einem Algorithmus programmatisch beibringen könnte. Bis noch vor einigen Jahrzehnten hat man so versucht Bildklassifikation durch manuell entwickelte Algorithmen zu erreichen. Die automatisierte Klassifikation anhand von vorgegebenen und vorklassifizierten Bildern und dem automatisierten Erstellen von Modellen war ein neuer Schritt in eine neue Vorgehensweise. Die dabei entwickelten neuronale Netze spielten eine gewaltige Rolle und änderten dramatisch die Art der Herangehensweise! Mittlerweile ist die Bilderkennung ein weit verbreitetes Anwendungsgebiet des maschinellen Lernens. Häufig werden für Bilder sogenannte "Convolutional Neural Networks\footnote{Convolutional neural network, Wikipedia contributors, January 31, 2020, \url{https://en.wikipedia.org/wiki/Convolutional_neural_network}}" or "ConvNets" are often used for images.}
			\en{For a long time, the automatic recognition of objects, people and scenes in images by computers was considered impossible. The complexity seemed too great to be programmatically taught to an algorithm. Until a few decades ago, attempts were made to achieve image classification by manually developed algorithms. Automated classification based on given and pre-classified images and the automated creation of models was a new step into a new approach. The neural networks developed in this process played a huge role and dramatically changed the way of approach! In the meantime, image recognition has become a widespread application area of machine learning. So-called "Convolutional Neural Networks\footnote{Convolutional neural network, Wikipedia contributors, January 31, 2020, \url{https://en.wikipedia.org/wiki/Convolutional_neural_network}}" or "ConvNets" are often used for images.}

			\de{Der Bildklassifizierungsalgorithmus nimmt ein Bild als Eingabe und klassifiziert es in eine der Ausgabekategorien. Deep Learning hat den Bereich der Bildklassifizierung revolutioniert und großartige Ergebnisse erzielt. Verschiedene Deep Learning Netzwerke, wie ResNet, DenseNet, Inception, etc. wurden als hochpräzise Netzwerke für die Bildklassifikation entwickelt. Gleichzeitig wurden Bilddatensätze angelegt, um getaggte Bilddaten zu erfassen. Diese werden jetzt vorrangig dazu verwendet um bestehende Netzwerke zu trainieren und alljährliche Challenges zu veranstalten, welche sich mit den bisher bekannten und schon entwickelten Modellgenauigkeiten messen. ImageNet ist ein solch großer Datensatz mit mehr als 11 Millionen Bildern und über 11.000 Kategorien. Wenn ein Netzwerk einmal mit ImageNet-Daten trainiert wurde, kann es durch einfache Neuanpassung oder Optimierung mit anderen Datensätzen verallgemeinert werden. Bei diesem Transfer-Lernansatz wird ein Netzwerk mit Gewichten initialisiert, welche aus einem zuvor trainierten Netzwerk stammen. Dieses zuvor initialisierte Netzwerk wird nun für eine neue Bildklassifikationsaufgabe lediglich entsprechend angepasst.}
			\en{The image classification algorithm takes an image as input and classifies it into one of the output categories. Deep Learning has revolutionized the field of image classification and has achieved great results. Various Deep Learning networks, such as ResNet, DenseNet, Inception, etc. have been developed as high-precision networks for image classification. At the same time, image data sets were created to capture tagged image data. These are now primarily used to train existing networks and to organize annual challenges that compete with the model accuracies already known and developed. ImageNet is such a large data set with more than 11 million images and over 11,000 categories. Once a network has been trained with ImageNet data, it can be generalized with other data sets by simple re-compilation or optimization. In this transfer learning approach, a network is initialized with weights that come from a previously trained network. This previously initialized network is now simply adapted for a new image classification task.}

			\de{Die hier zugrunde liegende Arbeit beschäftigt sich hauptsächlich mit überwachtem Lernen, bei dem ein mathematisches Modell aufgrund bestehender bekannter Datensätze trainiert wird. Das Ziel des trainierten Modells ist es dabei auch für unbekannte Bilder bestmögliche Vorhersagen zu treffen. Diese bekannten Datensätze werden meist händisch erstellt (Ontologe), automatisiert aufgrund bekannter Tatsachen bestimmt oder auch in einem halbautomatischen Prozess ermittelt.}
			\en{The underlying work here is mainly concerned with supervised learning, in which a mathematical model is trained based on existing known data sets. The goal of the trained model is to make the best possible predictions even for unknown images. These known data sets are usually created manually (ontologist), automatically determined based on known facts or determined in a semi-automatic process.}
			
		% -------------------- %
		% Deductive approach %
		% -------------------- %
		\subsubsection{Deductive approach}
		\label{sec:section_deductive_approach}
			\de{Seit den späten 1960er Jahren hat man versucht, Bilder mit selbstgeschriebenen Algorithmen zu klassifizieren. Dieser Teil der Computer Vision beschäftigt sich mit Techniken wie Bildentstehung, Bildbearbeitung und Bildsegmentierung. Im Bereich der Bildverarbeitung reihen sich bekannte Verfahren wie Kantenerkennungen, Merkmalsdetektoren, Randverknüpfungen, Kontrastverbesserungen, etc.\footnote{Szeliski, R.: Computer Vision: Algorithms and Applications,  Springer Science Business Media, 10 (2010)\label{springer_10}} Allen Techniken gemein ist die Verwendung des deduktiven Ansatzes. Beim deduktiven Ansatz erstellt man Regeln (Merkmalsdetektoren), welche das gewünschte Ergebnis vorhersagen sollen. Diese Regeln werden vorgegeben und beschrieben und erlauben damit später eine Klassifizierung von unbekannten Objekten. Da das Modell und sein Algorithmus hinreichend bekannt ist, wird dieses Verfahren White-Box-Verfahren genannt.}
			\en{Since the late 1960s, attempts have been made to classify images with self-written algorithms. This part of Computer Vision deals with techniques such as image creation, image processing and image segmentation. In the field of image processing, well-known techniques such as edge detection, feature detectors, edge linking, contrast enhancement, etc. are used\footnote{Szeliski, R.: Computer Vision: Algorithms and Applications,  Springer Science Business Media, 10 (2010)\label{springer_10}}. Common to all techniques is the use of the deductive approach. With the deductive approach, one creates rules (feature detectors) which are supposed to predict the desired result. These rules are given and described and thus allow later classification of unknown objects. Since the model and its algorithm are sufficiently well known, this procedure is called a white-box procedure.}

			\begin{figure}[H]
				\begin{center}
					\scalebox{1.0}{\includetex{deductive_approach}}
				\end{center}
				\caption{Deductive approach}
				\label{fig:overview_deductive_approach}
			\end{figure}

			\de{}

		% -------------------- %
		% Inductive approach %
		% -------------------- %
		\subsubsection{Inductive approach}
		\label{sec:section_inductive_approach}
			\de{Der induktive Ansatz hingegen verfolgt einen anderen Ansatz Bilder zu klassifizieren. Das Ziel ist nicht die Vorgabe einer Regel, sondern das Vorgehen aus schon bekannten einzelnen Objekten eine Regel (Modell) automatisiert zu erlernen. Ein Modell meist eine komplexe Funktion und eine mathematische Abbildung eines Raumes (VC Dimension\footnote{Vapnik–Chervonenkis dimension, Wikipedia contributors, January 31, 2020, \url{https://en.wikipedia.org/wiki/Vapnik\%E2\%80\%93Chervonenkis_dimension}}), in welcher einzelne Objekte mit ihren Eigenschaften abgebildet und getrennt werden können. Das Modell wird Stück für Stück den bekannten Objekten so angepasst, dass der Eingabewert dem Ausgabewert entspricht bzw. weitgehend entspricht (Backpropagation). Das Ziel ist es mit diesem Modell eine Funktion zu erstellen, welche in der Lage ist auch unbekannte Objekte bestmöglichst zu klassifizieren. Da der Raum dieses Modell meist fern der Vorstellungskraft und der Erklärungsmöglichkeit liegt, wird dieses Verfahren auch Black-Box-Verfahren genannt. Der hier beschriebene Vorgang wird meist bei jeder Art von \hyperref[sec:section_supervised_classification]{überwachtem Lernen} angewendet und ist ein Teil des \hyperref[sec:section_machine_learning]{maschinellem Lernens}.}
			\en{The inductive approach, on the other hand, takes a different approach to classifying images. The goal is not to specify a rule, but to learn a rule (model) automatically from already known individual objects. A model is usually a complex function and a mathematical representation of a space (VC dimension\footnote{Vapnik–Chervonenkis dimension, Wikipedia contributors, January 31, 2020, \url{https://en.wikipedia.org/wiki/Vapnik\%E2\%80\%93Chervonenkis_dimension}}), in which individual objects with their properties can be mapped and separated. The model is adapted piece by piece to the known objects in such a way that the input value corresponds to the output value or corresponds to a large extent (backpropagation). The goal is to create a function with this model, which can classify unknown objects in the best possible way. Because the space of this model is mostly far away from the imagination and the possibility of explanation, this procedure is also called a black-box procedure. The procedure described here is mostly used for any kind of \hyperref[sec:section_supervised_classification]{supervised learning} and is a part of \hyperref[sec:section_machine_learning]{machine learning}.}

			\begin{figure}[H]
				\begin{center}
					\scalebox{1.0}{\includetex{inductive_approach}}
				\end{center}
				\caption{Inductive approach}
				\label{fig:overview_inductive_approach}
			\end{figure}

		% -------------------- %
		% Balanced training data set %
		% -------------------- %
		\subsubsection{Balanced training data set}
		\label{sec:section_balanced_training_data_set}
			\de{Neuronale Netze haben in den letzten Jahren enorme Fortschritte im Bereich der Mustererkennung gemacht. Dabei ist ein entscheidender Faktor, dass die Daten zum Lernen eine hohe Qualität und eine einfache Verarbeitung für das Netzwerk aufweisen müssen. Falsch klassifizierte oder irrelevante Daten könnten dazu führen, dass das Netzwerk was Falsches lernt. Das gilt auch für eine nicht vorhandene bzw. nicht geeignete Vorverarbeitung.}
			\en{Neural networks have made enormous progress in the field of pattern recognition in recent years. A decisive factor is that the data for learning must be of high quality and easy for the network to process. Wrongly classified or irrelevant data could cause the network to learn something wrong. This also applies to non-existent or unsuitable pre-processing\autocite{osinga2019data}.}

			\de{Mit dem Beginn eines Klassifizierungsprojektes stellt sich die Frage was genau man klassifizieren möchte und wie umfangreich die Klassifzierung ausfallen soll. Angenommen man möchte verschiedene Klassen von Essen identifizieren, so könnten das Klassen wie Pizza, Burger, Donuts und Lasagne sein (etc.). Zu diesen Klassen benötigt man nun eine große Anzahl an Bildern. Diese Daten sollten im Idealfall die Realität möglichst gut widerspiegeln. Eine große Variation ist von Vorteil (ausbalancierter Datenset): verschiedene Blickwinkel, Größe, Position, Farbhelligkeiten, Variationen, Anzahl, etc. Bilder von z.B. nur einer Farbhelligkeit oder nur einem Blickwinkel sollten vermieden werden. Sind die Daten nicht ausbalanciert, so müssen diese entsprechend korrigiert werden: z.B. durch Hinzufügen weiterer Daten, Bildverarbeitung oder durch Entfernen von Daten, welche für eine Unausgewogenheit sorgen. Weiterhin sollten die ausgewählten Klassen untereinander klar optisch trennbar sein. Sind sich zwei Klassen optisch sehr ähnlich und selbst durch einen Menschen nicht wirklich unterscheidbar, sollte darüber nachgedacht werden diese zusammenzufassen (z.B. "burger" und "veggie burger"):}
			\en{With the beginning of a classification project, the question arises what exactly one wants to classify and how extensive the classification should be. Assuming one wants to identify different classes of food, this could be classified as pizza, burgers, doughnuts and lasagna (etc.). For these classes, one now needs a large number of images. Ideally, this data should reflect reality as well as possible. A large variation is advantageous (balanced data set): different viewing angles, size, position, colour brightness, variations, number, etc. Images of e.g. only one colour brightness or only one viewing angle should be avoided. If the data are not balanced, they must be corrected accordingly: e.g. by adding further data, image processing or by removing data that causes an imbalance. Furthermore, the selected classes should be clearly optically separable from each other. If two classes are visually very similar and not really distinguishable even by a human, consideration should be given to combining them (e.g. "burger" and "veggie burger"):}

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.19\textwidth]{images/data/burger/burger28.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/burger/burger77.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/burger/burger89.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/burger/burger162.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/burger/burger449.jpeg}
				\caption[Example pictures of a burger class]{Example pictures of a burger class}
				\label{fig:class_burger}
			\end{figure}

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.19\textwidth]{images/data/donut/donut116.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/donut/donut155.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/donut/donut176.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/donut/donut205.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/donut/donut440.jpg}
				\caption[Example pictures of a donut class]{Example pictures of a donut class}
				\label{fig:class_donut}
			\end{figure}

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.19\textwidth]{images/data/pizza/pizza73.png}
				\includegraphics[width=0.19\textwidth]{images/data/pizza/pizza76.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/pizza/pizza92.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/pizza/pizza108.jpg}
				\includegraphics[width=0.19\textwidth]{images/data/pizza/pizza239.jpg}
				\caption[Example pictures of a pizza class]{Example pictures of a pizza class}
				\label{fig:class_pizza}
			\end{figure}

			\de{An Daten zu gelangen ist oftmals nicht so einfach. Jede Datenquelle hat ihre eigenen Besonderheiten. Eine Möglichkeit an Daten zu gelangen wäre ein automatische Crawling von Bilddatenbanken, Suchmaschinen oder Rezensionen, in welchen Bilder vorkommen. Ein gewisses Maß an Kreativität ist von Vorteil:}
			\en{Accessing data is often not that easy. Every data source has its own special features. One way to access data would be an automatic crawling of image databases, search engines or reviews in which images appear. A certain amount of creativity is advantageous:}

			\begin{itemize}
				\item Google
				\item Bing
				\item Flickr
				\item TripAdvisor
				\item etc.
			\end{itemize}
			
			\de{Die wahrscheinlich teuerste Variante an Daten zu gelangen ist die händische Suche und Klassifizierung durch z.B. einen Ontologen. Dieser beurteilt und sucht verschiedenen Bilder und ordnet diese händisch in die entsprechenden Klassen ein. Auch eine kombinierte Variante ist möglich und wahrscheinlich zu bevorzugen: Automatisches Crawling und händisches aussortieren falscher, ungünstiger oder irrelevanter Bilder.}
			\en{Probably the most expensive way to obtain data is to search and classify them manually, e.g. by an ontologist. The ontologist evaluates and searches for different images and manually classifies them in the appropriate classes. A combined variant is also possible and probably preferable: automatic crawling and manual sorting out of incorrect, unfavorable or irrelevant images.}

		% -------------------- %
		% Training, test and evaluation data set %
		% -------------------- %
		\subsubsection{Training, test and evaluation data set}
		\label{sec:section_training_test_and_evaluation}
			\de{Vor dem Beginn mit dem Training von ausbalancierten Bildern müssen diese in einen Trainings-, einen Test- und eventuell in einen Validierungsdatensatz aufgeteilt werden. Dies ist notwendig, da neuronale Netze zu einem Teil nicht verallgemeinern, sondern auswendig lernen werden (overfitting\footnote{``Overfitting'', Wikipedia contributors, January 31, 2020, \url{https://en.wikipedia.org/wiki/Overfitting}}). Die Idee ist es mit einem Trainingsdatensatz zu trainieren, während mit dem Validierungsdatensatz die Allgemeingültigkeit des Netzes und seiner Parameter überwacht wird. Anhand der Ergebnisse werden zur Laufzeit Anpassungen vorgenommen. Da die Anpassung der Parameter anhand der Testdaten vorgenommen wird, gibt es noch einen unabhängigen Testdatensatz, welcher eine erneute Überprüfung des Modells auf bisher unbeteiligte Daten vornimmt. Dieser stellt sicher, dass nicht versehentlich Hyperparameter nur speziell für den Validierungsdatensatz optimiert werden\autocite{osinga2019data}. Die Verwendung des Testdatensatz ist optional und simuliert das Modell unter realen Bedingungen. Ist die Anzahl der Daten begrenzt und kann dieser Datensatz z.B. auch dem Trainingsdatensatz hinzugefügt werden. In dieser Arbeit wird auf den Testdatensatz verzichtet und sämtliche Auswertungen beziehen sich auf den Validierungsdatensatz.}
			\en{Before starting the training of balanced images, they must be divided into a training, a test and possibly a validation data set. This is necessary because neural networks will not generalize to some extent, but will learn by heart (overfitting\footnote{``Overfitting'', Wikipedia contributors, January 31, 2020, \url{https://en.wikipedia.org/wiki/Overfitting}}). The idea is to train with a training data set, while the validation data set is used to monitor the general validity of the network and its parameters. Based on the results, adjustments are made at runtime. Since the adjustment of the parameters is carried out using the test data, there is also an independent test data set, which carries out a renewed check of the model for previously uninvolved data. This ensures that hyperparameters are not inadvertently optimized for the validation data set only\autocite{osinga2019data}. The use of the test data set is optional and simulates the model under real conditions. If the number of data is limited, this data record can also be added to the training data record, for example. In this thesis, the test data set is not used and all evaluations refer to the validation data set.}

			\de{Eine optimale Aufteilung des Trainings- und Validierungsdatensatzes ist abhängig von dem vorliegenden Klassifizierungsproblem und die Anzahl der Daten, welche zur Verfügung stehen. In dieser Arbeit wird ein Verhältnis aus 80 Prozent Trainingsdaten und 20 Prozent Validierungsdaten verwendet, sofern nicht anders angegeben.}
			\en{An optimal division of the training and validation data set depends on the existing classification problem and the amount of data available. In this paper a ratio of 80 percent training data and 20 percent validation data is used, unless otherwise stated.}

			\de{\hl{Hier muss die Frage noch geklärt werden warum 80 Prozent Trainingsdaten und 20 Prozent Validierungsdaten verwendet werden. Gibt es ein Paper bzw. eine Studie dazu? Oder ist noch ein Test erforderlich?}}
			\en{\hl{The question of why 80 percent training data and 20 percent validation data are used remains to be clarified. Is there a paper or a study on this? Or is another test required?}}			

		% -------------------- %
		% Methods of machine learning %
		% -------------------- %
		\subsubsection{Methods of machine learning}
		\label{sec:section_methods_of_machine_learning}
			\de{Je nach Art und Vorgehensweise der Überwachung des Trainings lassen sich verschiedene Machine-Learning-System einordnen. Dabei wird unterschieden, welche Art von Daten uns vorliegen oder diese selbst bestimmt werden müssen.}
			\en{Different machine learning systems can be classified according to the type and procedure of monitoring the training. A distinction is made as to which type of data is available or has to be determined by the user.}

			% -------------------- %
			% Supervised learning %
			% -------------------- %
			\paragraph{Supervised learning}
			\label{sec:section_supervised_learning}
				\de{Das überwachte Lernen bezieht sich auf ein maschinelles Lernen mit bekannten Trainingsdatensätzen (siehe auch Kapitel \hyperref[sec:section_inductive_approach]{``induktiver Ansatz''}). Der Lernprozess wiederrum bezieht sich auf die Fähigkeit einer künstlichen Intelligenz, Regelmäßigkeiten und Muster zu reproduzieren. Die Ergebnisse sind durch Naturgesetze oder Expertenwissen bekannt und werden für die Lehre des Systems verwendet, in dem ein Trainingsset erstellt wird, welcher die gewünschten Lösungen enthält. Man nennt dies auch gelabelte Daten. Der Lernalgorithmus versucht nun epochenweise eine Hypothese zu finden, die eine möglichst genaue Vorhersagen auf unbekannten Daten ermöglicht. Eine Hypothese ist in diesem Fall ein Bild, das jedem Eingabewert (das Bild selbst) den angenommenen Ausgabewert (die vorhergesagte Klasse) zuordnet. Diese Arbeit macht ausgiebigen Gebrauch von überwachtem Lernen.}
				\en{Supervised learning refers to machine learning with known training data sets (see also chapter \hyperref[sec:section_inductive_approach]{``inductive approach''}). The learning process in turn refers to the ability of an artificial intelligence to reproduce regularities and patterns. The results are known by laws of nature or expert knowledge and are used to teach the system by creating a training set containing the desired solutions. This is also called labelled data. The learning algorithm now tries to find a hypothesis epoch by epoch, which allows the most accurate predictions on unknown data. A hypothesis in this case is an image that assigns the assumed output value (the predicted class) to each input value (the image itself). This work makes extensive use of supervised learning.}
	
				\de{Beim Überwachten Lernen wird einer Klassifizierungsfunktion (meist ein künstliches neuronales Netz) ein Eingangsvektor zugeführt. Der Eingangsvektor erzeugt mit Hilfe der Klassifizierungsfunktion einen Ausgabevektor, die dieses neuronale Netz in seinem aktuellen Zustand produziert\footnote{Das neuronale Netzwerk besteht aus vielen (meist Millionen) Parametern, welche während des Lernprozesses angepasst werden können, um den Fehler zu minimieren.}. Dieser Wert wird mit dem Wert verglichen, den es eigentlich ausgeben soll. Der Vergleich des Soll- und Istzustandes gibt Auskunft wie und in welcher Form Änderungen am Netzwerk vorgenommen werden müssen, um dem Istzustand weiter anzugleichen und den Fehler zu minimieren. Für künstliche neuronale Netzwerke ohne versteckter Schicht (einlagiges Perzeptron\footnote{``Perceptron'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Perceptron}}) kann die Delta-Regel\footnote{``Least mean squares'' filter also known as ``delta rule'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Least_mean_squares_filter}} für die Korrektur vorgenommen werden. Bei Netzwerken mit einer oder mehreren versteckten Schichten verwendet man Backpropagation\footnote{``Backpropagation'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Backpropagation}} um den Fehler zu minimieren. Backpropagation ist eine Verallgemeinerung der Delta-Regel.}
				\en{In supervised learning, an input vector is fed to a classification function (usually an artificial neural network). The input vector generates an output vector using the classification function, which produces this neural network in its current state\footnote{The neural network consists of many (usually millions) parameters, which can be adjusted during the learning process to minimize the error.}. This value is compared with the value that it should actually output. The comparison of the nominal and actual state provides information on how and in what form changes must be made to the network in order to further approximate the actual state and minimize the error. For artificial neural networks without a hidden layer (single-layer perceptron\footnote{``Perceptron'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Perceptron}}), the delta rule\footnote{``Least mean squares'' filter also known as ``delta rule'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Least_mean_squares_filter}} for correction can be applied. For networks with one or more hidden layers backpropagation\footnote{``Backpropagation'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Backpropagation}} is used to minimize the error. Backpropagation is a generalization of the delta rule.}
				
				\de{Das neuronale Netzwerke ist nur ein Algorithmus aus der Kategorie der überwachten Lernalgorithmen. Der Vollständigkeit hier noch eine Liste von weiteren Verfahren:}
				\en{The neural network is only one algorithm from the category of supervised learning algorithms. For completeness here is a list of further algorithms:}
				
				\begin{itemize}
					\item k-nearest neighbors\footnote{``k-nearest neighbors algorithm'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm}}
					\item Linear regression\footnote{``Linear regression'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Linear_regression}}
					\item Logistic regression\footnote{``Logistic regression'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Logistic_regression}}
					\item Support-vector machine\footnote{``Support-vector machine'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Support-vector_machine}}
					\item Random forest\footnote{``Random forest'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Random_forest}}
					\item etc.
				\end{itemize}
		
			% -------------------- %
			% Unsupervised learning %
			% -------------------- %
			\paragraph{Unsupervised learning}
			\label{sec:section_unsupervised_learning}
				\de{Beim unüberwachtem Lernen versucht man auch ohne gelabelte Daten an eine Kenntnis von Mustern zu erlangen. Angenommen man hätte mehrere Bilder von Burgern, Pizza und Donuts, welche sich unsortiert in einem Datenset befinden. Das unüberwachte Lernen versucht nun Ähnlichkeiten zu finden, um diese Bilder zu gruppieren. Im besten Fall erhält man am Ende drei unbenannte Gruppen \(A\), \(B\) und \(C\). Analysten werden sich diese Gruppen im Nachgang genauer anschauen und ein Fazit daraus ziehen, sofern dies möglich ist: Gruppe \(A\) trägt dann den Namen Burger, Gruppe \(B\) Pizzen, etc.}
				\en{In unsupervised learning, one tries to gain knowledge of patterns even without labelled data. Suppose one has several pictures of burgers, pizza and doughnuts, which are unsorted in a data set. Unsupervised learning now tries to find similarities in order to cluster these images. In the best case one gets three unnamed groups \(A\), \(B\) and \(C\) at the end. Analysts will take a closer look at these groups afterwards and draw a conclusion if possible:  Group \(A\) is then called Burger, Group \(B\) Pizzas, etc.}
				
				\de{\noindent Folgende unüberwachte Lernalgorithmen können zur Gruppierung verwendet werden:}
				\en{\noindent The following unsupervised learning algorithms can be used for clustering:}
				
				\begin{itemize}
					\item k-means\footnote{``k-means clustering'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/K-means_clustering}}
					\item Hierarchical clustering\footnote{``Hierarchical clustering'', Wikipedia contributors, February 2, 2020, \url{https://en.wikipedia.org/wiki/Hierarchical_clustering}}
					\item Expectation–maximization \footnote{``Expectation–maximization algorithm'', Wikipedia contributors, February 2, 2020, \url{Expectation–maximization algorithm}}
					\item etc.
				\end{itemize}
			
				\de{Eine Technik, die hierarchische Clusteranalyse (siehe Kapitel \hyperref[sec:section_validation_hierarchical classification]{``Hierarchical classification''}), wird später verwendet um die Einführung von Hierarchien zu erleichtern. Für die allgemeine Analyse, den Finden von optimalen Parametern für das Lernen von Modellen, wird diese Art des Lernens in dieser Arbeit nicht verwendet.}
				\en{One technique, hierarchical clustering (see chapter \hyperref[sec:section_validation_hierarchical classification]{``Hierarchical classification''}), is used later to facilitate the introduction of hierarchies. For the general analysis, the finding of optimal parameters for learning models, this kind of learning is not used in this thesis.}

		\label{}
			% -------------------- %
			% Reinforcement learning %
			% -------------------- %
			\paragraph{Reinforcement learning}
			\label{sec:section_reinforcement_learning}
				\de{Reinforcement learning\footnote{``Reinforcement learning'', Wikipedia contributors, January 31, 2020, \url{https://en.wikipedia.org/wiki/Reinforcement_learning}} ist eine Art des maschinellen Lernens, bei der ein Agent selbstständig die bestmögliche Strategie für die Erreichung eines Zieles erlernt. Für die Erreichung des Ziels sind Aktionen notwendig, welche zu bestimmten Zeitpunkten Belohnungen hervorbringen. Diese Belohnungen können auch negative sein (Bestrafung). Anhand dieser Belohnungen gilt es in Summe den bestmöglichen Belohnungswert zu erzielen. Bei der Klassifizierung von Bildern ist diese Art des Lernens nicht relevant, weshalb hier auch nicht weiter darauf eingegangen wird.}
				\en{Reinforcement learning\footnote{``Reinforcement learning'', Wikipedia contributors, January 31, 2020, \url{https://en.wikipedia.org/wiki/Reinforcement_learning}} is a type of machine learning in which an agent independently learns the best possible strategy for achieving a goal. To achieve the goal, actions are necessary which produce rewards at certain points in time. These rewards can also be negative (punishment). Based on these rewards, the aim is to achieve the best possible reward value. This type of learning is not relevant for the classification of images, which is why it will not be discussed further here.}



		% -------------------- %
		% Classification Metrics %
		% -------------------- %
		\subsubsection{Classification metrics and confusion matrix}
			\de{Die Wahl der richtigen Metrik ist bei der Bewertung von Modellen des maschinellen Lernens von entscheidender Bedeutung. Metriken werden zur Überwachung und Messung der Leistung eines Modells während des Trainings und des Tests verwendet. Im nachfolgenden werden einige wichtige Metriken erklärt.}
			\en{Choosing the right metric is crucial in evaluating machine learning models. Metrics are used to monitor and measure the performance of a model during training and testing. Some important metrics are explained below.}

			% -------------------- %
			% Confusion Matrix %
			% -------------------- %
			\paragraph{Confusion matrix}
			 	\de{Die Confusion Matrix ist eine spezielle quadratische Matrix auf dem Gebiet des maschinellen Lernens, das die Visualisierung der Leistung eines Vorhersage-Modells ermöglicht. Jede Zeile der Matrix repräsentiert die tatsächliche Klasse, während jede Spalte die Anzahl oder eine Zahlenangaben in Prozent der vorhergesagten Klasse angibt (oder umgekehrt)\footnote{``Confusion matrix'', Wikipedia contributors, February 5, 2020, \url{https://en.wikipedia.org/wiki/Confusion_matrix}}:}
			 	\en{The Confusion Matrix is a special quadratic matrix in the field of machine learning that allows the visualization of the performance of a predictive model. Each row of the matrix represents the actual class, while each column indicates the number or a numerical value as a percentage of the predicted class (or vice versa)\footnote{``Confusion matrix'', Wikipedia contributors, February 5, 2020, \url{https://en.wikipedia.org/wiki/Confusion_matrix}}:}

				\begin{table}[htb]
					\centering
					\scalebox{0.75}{
						{\def\arraystretch{2}\tabcolsep=5pt
							\begin{tabular}{cc|c|c|c|c|}
								\cline{3-6}
								& & \multicolumn{4}{c|}{\textbf{predicted}} \\ \cline{3-6} 
								& & \boldmath\(class_1\) & \boldmath\(class_2\) & \textbf{\ldots} & \boldmath\(class_n\) \\ \hline
								\multicolumn{1}{|l|}{\multirow{4}{*}{\rotatebox{90}{\textbf{actual}}}} & \boldmath\(class_1\) & \(TP\)                  & \multicolumn{3}{c|}{\(FN\)} \\ \cline{2-6} 
								\multicolumn{1}{|c|}{} & \boldmath\(class_2\) & \multirow{3}{*}{\(FP\)} & \multicolumn{3}{c|}{\multirow{3}{*}{\(TN\)}} \\ \cline{2-2}
								\multicolumn{1}{|c|}{} & \textbf{\ldots} & & \multicolumn{3}{c|}{}                   \\ \cline{2-2}
								\multicolumn{1}{|c|}{} & \boldmath\(class_n\) & & \multicolumn{3}{c|}{}                   \\ \hline
							\end{tabular}
						}
					}
					\captionof{table}{Confusion matrix}\label{tbl:table_confusion_matrix}
				\end{table}

			 	\de{Zum Schluß besitzt die Confusion Matrix folgenden Aufbau, wobei die Anzahl von Elementen in der Klasse \(C_{i,P}\) vorhergesagt wurde, obwohl (\(\boldsymbol{\cong}\)) es hätte Klasse \(C_{j,A}\) sein müssen:}
				\en{Finally, the Confusion Matrix has the following structure, where the number of elements in the class \(C_{i,P}\) was predicted although (\(\boldsymbol{\cong}\)) it should have been class \(C_{j,A}\):}

				\begin{equation}
					\boldsymbol{M}_{confusion} = \begin{bmatrix}
						\#C_{1,P}\boldsymbol{\cong} C_{1,A} & \dots & \#(C_{n,P}\boldsymbol{\cong} C_{1,A}) \\
						\vdots & \ddots & \vdots \\
						\#(C_{1,P}\boldsymbol{\cong} C_{n,A}) & \dots & \#(C_{n,P}\boldsymbol{\cong} C_{n,A})
					\end{bmatrix} = (a_{nn})
				\end{equation}

			% -------------------- %
			% Accuracy %
			% -------------------- %
			\paragraph{Accuracy}
				\de{Die \textbf{Top-1-Genauigkeit} ist wahrscheinlich die wichtigste Genauigkeit. Sie sagt sagt aus, zu wieviel Prozent die jeweils beste Aussage des Modells auf die Daten des Validierungssets mit der erwarteten Klasse übereinstimmt.}
				\en{\textbf{Top-1 accuracy} is probably the most important accuracy. It tells one the percentage of the model's best prediction of the data in the validation set that matches the expected class.}

				\begin{equation}
					Accuracy = {{TP + TN} \over {TP + TN + FP + FN}} = {{\sum_{i,j=1}^{n} a_{ij}} \over {\sum_{i=1}^{n} \sum_{j=1}^{n} a_{ij}}} = {{Correct_{all}} \over {CorrectPossible_{all}}}
				\end{equation}

				\de{Die \textbf{Top-5-Genauigkeit} ist eine weitere Genauigkeitsangabe. Jedoch wird hier nicht nur der beste Treffer einbezogen, sondern auch die nächsten weiteren vier. Sobald die richtige Klasse innerhalb der ersten fünf vorhergesagten Klassen gefunden werden kann, so ist auch diese Vorhersage wahr:}
				\en{The \textbf{Top 5 Accuracy} is another accuracy specification. However, not only the best hit is included here, but also the next four. As soon as the correct class can be found within the first five predicted classes, this prediction is also true:}
				
				\begin{equation}
					Accuracy_{top-5} = {{CorrectWithinTheBest5Classes_{all}} \over {CorrectPossible_{all}}}
				\end{equation}

				\de{Die Genauigkeit des gesamten Modells ist eine gute Aussagekraft über die Leistungsfähigkeit des Modells. Ein Problem tritt jedoch in Extremfällen auf, bei denen nicht mehr zuverlässig Annahmen gemacht werden können. Zum Beispiel wenn man es mit einem unbalancierten Datensatz zu tun hat\autocite{geron2017supervisedlearningConfusionMatrix}. Beispiel: Angenommen man hätte ein Modell, dass immer die Klasse \(class_1\) vorhersagt. Die Klasse \(class_1\) besteht aus 9990 Elementen und von den anderen Klassen \(class_2\) bis \(class_n\) hat man genau 10. Dann sieht die Confusion Matrix wie folgt aus:}
				\en{The accuracy of the entire model is a good indication of the performance of the model. However, a problem occurs in extreme cases where assumptions can no longer be made reliably. For example, if one is working with an unbalanced dataset\autocite{geron2017supervisedlearningConfusionMatrix}. Example: Suppose one has a model that always predicts the class \(class_1\). The class \(class_1\) consists of 9990 elements and from the other classes \(class_2\) to \(class_n\) one has exactly 10. Then the Confusion Matrix looks like this:}

				%\clearpage
				\begin{table}[htb]
					\centering
					\scalebox{0.75}{
						{\def\arraystretch{2}\tabcolsep=5pt
							\begin{tabular}{cc|c|c|c|c|}
								\cline{3-6}
								& & \multicolumn{4}{c|}{\textbf{predicted}} \\ \cline{3-6} 
								& & \boldmath\(class_1\) & \boldmath\(class_2\) & \textbf{\ldots} & \boldmath\(class_n\) \\ \hline
								\multicolumn{1}{|l|}{\multirow{4}{*}{\rotatebox{90}{\textbf{actual}}}} & \boldmath\(class_1\) & \(TP = 9990\)                  & \multicolumn{3}{c|}{\(FN = 0\)} \\ \cline{2-6} 
								\multicolumn{1}{|c|}{} & \boldmath\(class_2\) & \multirow{3}{*}{\(FP = 10\)} & \multicolumn{3}{c|}{\multirow{3}{*}{\(TN = 0\)}} \\ \cline{2-2}
								\multicolumn{1}{|c|}{} & \textbf{\ldots} & & \multicolumn{3}{c|}{}                   \\ \cline{2-2}
								\multicolumn{1}{|c|}{} & \boldmath\(class_n\) & & \multicolumn{3}{c|}{}                   \\ \hline
							\end{tabular}
						}
					}
					\captionof{table}{Confusion matrix example}\label{tbl:table_confusion_matrix}
				\end{table}

				\de{Die Modell-Genauigkeit ist ein ist in diesem Fall 99,9\%, obwohl es ein schlechtes Modell ist:}
				\en{The model accuracy in this case is 99.9\%, although it is a bad model:}

				\begin{equation}
					Accuracy = 99,9\%
				\end{equation}

				\de{Deswegen gibt es weitere Performance Metriken wie Precision, Recall und F-Measure.}
				\en{Therefore there are additional performance metrics like Precision, Recall and F-Measure.}

			% -------------------- %
			% Precision %
			% -------------------- %
			\paragraph{Precision}
				\de{Precision\footnote{``Precision and recall'', Wikipedia contributors, February 7, 2020, \url{https://en.wikipedia.org/wiki/Precision_and_recall}} sagt aus, wie zuverlässig die Aussage einer Vorhersage einer Klasse ist:}
				\en{Precision\footnote{``Precision and recall'', Wikipedia contributors, February 7, 2020, \url{https://en.wikipedia.org/wiki/Precision_and_recall}} expresses how reliable the statement of a prediction of a class is:}

				\begin{equation}
					Precision = {{Correct} \over {Actual}} = {{TP} \over {TP + FP}}
				\end{equation}

				\de{Oder genauer für die Klasse c:}
				\en{Or more precisely for class c:}

				\begin{equation}
					Precision_{@c} = {{a_{cc}} \over {\sum_{i=1}^{n} a_{ic}}}
				\end{equation}

			% -------------------- %
			% Recall %
			% -------------------- %
			\paragraph{Recall}
				\de{Recall\footnote{``Precision and recall'', Wikipedia contributors, February 7, 2020, \url{https://en.wikipedia.org/wiki/Precision_and_recall}} ist die Genauigkeit einer Klasse. Das bedeutet wie gut konnte die Klasse vorhergesehen werden:}
				\en{Recall\footnote{``Precision and recall'', Wikipedia contributors, February 7, 2020, \url{https://en.wikipedia.org/wiki/Precision_and_recall}} is the accuracy of a class. This means how well the class could be predicted:}

				\begin{equation}
					Recall = {{Correct} \over {CorrectPossible}} = {{TP} \over {TP + FN}}
				\end{equation}

				\de{Oder genauer für die Klasse c:}
				\en{Or more precisely for class c:}

				\begin{equation}
					Recall_{@c} = {{a_{cc}} \over {\sum_{i=1}^{n} a_{ci}}}
				\end{equation}

			% -------------------- %
			% F-Measure %
			% -------------------- %
			\paragraph{F-Measure}
				\de{F-Measure\footnote{``F1 score'', Wikipedia contributors, February 7, 2020, \url{https://en.wikipedia.org/wiki/F1_score}} kombiniert Präzision und Rückruf, wobei der Parameter \(\beta\) die Gewichtung darstellt:}
				\en{F-Measure\footnote{``F1 score'', Wikipedia contributors, February 7, 2020, \url{https://en.wikipedia.org/wiki/F1_score}} combines precision and recall, with the parameter \(\beta\) representing the weighting:}
				\begin{equation}
					F_\beta =
					(1 + \beta^2) \cdot {{Precision\cdot Recall}\over{\beta^2 \cdot Precision + Recall}} = 
					{{(1 + \beta^2) \cdot TP}\over{(1 + \beta^2) \cdot TP + \beta^2 \cdot FN + FP}}
				\end{equation}

				\de{Je höher der \(\beta\), desto mehr Wert wird auf Precision statt auf Recall gelegt. Das ist wichtig, wenn man mehr Wert auf die Qualität der Verhersage legt, als auf die Erkennungsgenauigkeit. Z.B. bei einer Vorhersage von Krankheiten: Zuordnung von Klasse Krank bei gesunde Menschen ist hier genauso fatal wie auch die Zuordnung von Klasse Gesund bei kranken Menschen (Obwohl Fall zwei fataler wäre als Fall eins). Mit einem Wert von \(\beta = 0,5\) erhält man eine Gleichverteilung beider Werte und wird F1 score genannt:}
				\en{The higher \(\beta\) the more importance is placed on precision instead of recall. This is important if one puts more importance on the quality of the prediction than on the accuracy of prediction. For example, when predicting diseases: assigning class ``ill'' to healthy people is just as fatal as assigning class ``healthy'' to sick people (although case two would be even more fatal than case one). A value of \(\beta = 0.5\) one gets an equal distribution of both values and is called F1 score:}

				\begin{equation}
					F_1 =
					{{2 \cdot Precision \cdot Recall}\over{Precision + Recall}} = 
					{{2 \cdot TP}\over{2 \cdot TP + FN + FP}}
				\end{equation}

			% -------------------- %
			% loss function %
			% -------------------- %
			\paragraph{Loss function}
			\label{sec:section_loss_function}
				\de{Da es sich hierbei um ein Klassifzierungsproblem und nicht um ein Regressionsproblem handelt, greift man bei der Vorhersage auf die Softmax-Regression zurück. Dabei wird nach jeder Vorhersage ein Vektor \(\lambda\) der Größe \(n\) zurückgegeben, wobei \(n\) der Anzahl der zu unterscheidenden Klassen entspricht\autocite{geron2017supervisedlearningDecisionBoundaries}. Jeder einzelne \(\hat{p}\) Wert entspricht dabei der Wahrscheinlichkeit, dass es die Klasse \(class_n\) ist:}
				\en{Since this is a classification problem and not a regression problem, the prediction is based on softmax regression. After each prediction, a vector \(\lambda\) of the size \(n\) is returned, where \(n\) corresponds to the number of classes to be distinguished\autocite{geron2017supervisedlearningDecisionBoundaries}. Each individual \(\hat{p}\) value corresponds to the probability that it is class \(class_n\):}

				\begin{equation}
					\lambda = 
					\left(
						\begin{array}{c}
						\hat{p}_{1}\\
						\hat{p}_{2}\\
						\vdots\\
						\hat{p}_{n}\\
						\end{array}
					\right)
					\quad\Biggl\lvert \quad \sum_{i=1}^n \hat{p_i} = 1
				\end{equation}

				\de{Der erwartete Wert der Parameterfunktion und somit der aktuellen Klasse wird als One Hot Vektor zurückgegeben. Der Wert 1 entspricht der erwarteten Klasse. Alle anderen Klassen geben 0 zurück. Dies nennt man auch One Hot Encoding:}
				\en{The expected value of the parameter function and thus of the current class is returned as a one hot vector. The value 1 corresponds to the expected class. All other classes return 0. This is also called one hot encoding:}

				\begin{equation}
					g(\vartheta_2) = 
					\left(
						\begin{array}{c}
						0\\
						1\\
						\vdots\\
						0\\
						\end{array}
					\right)
				\end{equation}

				\de{Die Verlustfunktion\footnote{``Verlustfunktion (Statistik)'', Wikipedia contributors, February 7, 2020, \url{https://de.wikipedia.org/wiki/Verlustfunktion_(Statistik)}} ordnet jeder Vorhersage einen Schaden zu, der durch den Vergleich mit dem wahren Wert bzw. Parameter entsteht. Dazu wird der Abstand der vorhergesagten Klasse zur wahren Klasse berechnet (sind sie gleich ist der Abstand 0). Bessert sich mit Anpassung des Modells (Lernvorgang) die Vorhersage aller vorhergesagten Klassen zu ihren wahren Klassen, so verkleinert sich auch der Wert der Verlustfunktion. Eine typische Verlustfunktion ist z.B. für einen r-dimensionalen Raum:}
				\en{The loss function\footnote{``Verlustfunktion (Statistik)'', Wikipedia contributors, February 7, 2020, \url{https://de.wikipedia.org/wiki/Verlustfunktion_(Statistik)}} assigns a loss to each prediction, which results from the comparison with the true value or parameter. For this purpose, the distance between the predicted class and the true class is calculated (if they are equal, the distance is 0).  If the adaptation of the model (learning process) improves the prediction of all predicted classes to their true classes, the value of the loss function is also reduced.  A typical loss function is e.g. for an r-dimensional space:}

				\begin{equation}
					L_r(\vartheta, \lambda) := \lVert \lambda - g(\vartheta) \rVert^r
				\end{equation}

				\de{\(\lambda\) stellt hierbei den geschätzte Wert und \(g(\vartheta)\) die Parameterfunktion dar, welche den realen Wert für \(\vartheta\) zurückliefert. Der mittlere Verlust auf den gesamten Datensatz mit \(n\) Elementen beträgt somit:}
				\en{\(\lambda\) represents the estimated value and \(g(\vartheta)\) the parameter function which returns the actual value for \(\vartheta\). The average loss on the entire data set with \(n\) elements is thus:}

				\begin{equation}
					\hat{L} = {{1}\over{k}}\sum_{i=1}^{k} L_r(\vartheta_i, \lambda_i)
				\end{equation}

		\subsection{Machine learning}
		\label{sec:section_machine_learning}
			\de{Maschinelles Lernen ist ein Oberbegriff für die künstliche Generierung von Wissen aus Erfahrung. Es verfolgt den Ansatz des induktiven Lernens (siehe auch Kapitel \hyperref[sec:section_inductive_approach]{``induktiver Ansatz''}).}
			\en{Machine learning is a generic term for the artificial generation of knowledge from experience. It follows the approach of inductive learning (see also chapter \hyperref[sec:section_inductive_approach]{``inductive approach''}).}

			\subsubsection{Artificial neural network}
				\de{Künstliche neuronale Netzwerke stellen Funktionen bereit, welche in der Lage sind hochkomplexe Daten im mehrdimensionalem Raum zu trennen. Bei großen und hochgradig komplexen Aufgaben, wie beispielsweise der Klassifizierung von Milliarden von Bildern, Sprach- und Texterkennungen, schneiden neuronale Netze meist besser ab, als andere Machine Learning Verfahren. Der erhebliche Zuwachs an Rechenkapizität seit den 1990ern ermöglicht das Trainieren großer neuronaler Netzwerke innerhalb eines sinnvollen Zeitraumes. Künstliche neuronale Netzwerke sind die Kernkomponente des Deep Learnings.}
				\en{Artificial neural networks provide functions that are able to separate highly complex data in multidimensional space. For large and highly complex tasks, such as the classification of billions of images, speech and text recognition, neural networks usually perform better than other machine learning methods. The significant increase in computational capacity since the 1990s allows the training of large neural networks within a reasonable period of time. Artificial neural networks are the core component of deep learning.}

				\de{Neuronale Netzwerke verarbeiten einen Eingabevektor und wandeln ihn in einen neuen Ausgabevektor um. Sie sind Netze aus vielen hintereinander und parallel geschalteten künstlichen Neuronen. Ein künstliches Neuron wiederrum wandelt einen Vektor in ein Skalar um, indem es die Eingänge \(\bar{x}\) mit den veränderlichen Parametern \(\bar{\omega}\) skaliert, aufsummiert und mit einem Bias \(b\) korrigiert (der Bias ist ebenso eine veränderliche Variable). Die Aktivierungsfunktion stellt sicher, dass aus dem Polynom ersten Grades (lineares Regressionsmodell) eine nichtlineare Funktion wird\footnote{``Aktivierungsfunktionen, ihre Arten und Verwendungsmöglichkeiten'', https://www.ai-united.de/, February 8, 2020, \url{https://www.ai-united.de/aktivierungsfunktionen-ihre-arten-und-verwendungsmoeglichkeiten/}}:}
				\en{Neural networks process an input vector \(\bar{x}\) and convert it into a new output vector \(\hat{\bar{x}}\). They are networks of many artificial neurons connected in series and parallel. An artificial neuron in turn converts a vector into a scalar by scaling and summing the inputs \(\bar{x}\) with the changeable parameters \(\bar{\omega}\) and correcting them with a bias \(b\) (the bias is also a changeable variable). The activation function ensures that the first degree polynomial (linear regression model) becomes a nonlinear function\footnote{``Activation functions, their types and uses'', https://www.ai-united.de/, February 8, 2020, \url{https://www.ai-united.de/aktivierungsfunktionen-ihre-arten-und-verwendungsmoeglichkeiten/}}:}

				\begin{figure}[H]
					\[
						\begin{array}{c}
							x_1\\
							x_2\\
							\color{white}\vdots\\
							x_n\\
							1\\
						\end{array}
						\begin{array}{c}
							\rightarrow\\
							\rightarrow\\
							\vdots\\
							\rightarrow\\
							\rightarrow\\
						\end{array}
						\begin{array}{c}
							\omega_1\\
							\omega_2\\
							\color{white}\vdots\\
							\omega_n\\
							b\\
						\end{array}
						\begin{array}{c}
							\diagdown\\
							\diagdown\\
							\\
							\diagup\\
							\diagup\\
						\end{array}
						\circled{${\sum\atop\ }\over{\ \atop step(z)}$}
						\longrightarrow h
						\quad\Biggl\lvert \quad h = step(z) = step(\bar{\omega}^\intercal \cdot \bar{x} + b)
					\]
					\caption{The construction of an artificial neuron.}
				\end{figure}

				\de{Das künstliche neuronale Netz baut sich aus vielen hintereinandergeschalteten Layern zusammen, welche wiederrum parallel geschaltete Neuronen enthalten:}
				\en{The artificial neural network is composed of many layers connected in series, which again contain neurons connected in parallel:}

				\begin{figure}[H]
					\centering
					\scalebox{0.75}{
						\tikzset{%
							every neuron/.style={
								circle,
								draw
						  	},
						  	every inputneuron/.style={
						  		circle,
						  		draw,
						  		minimum size=0.25cm
						  	},
						  	neuron 1/.style={
								circle,
								draw,
								minimum size=0.75cm,
								execute at begin node=\color{black}${\sum\atop\ }\over{\ \atop step(z)}$
						  	},
						  	neuron 2/.style={
								circle,
								draw,
								minimum size=0.75cm,
								execute at begin node=\color{black}${\sum\atop\ }\over{\ \atop step(z)}$
						  	},
						  	neuron missing/.style={
						  		draw=none,
						  		scale=2,
						  		text height=0.333cm,
						  		execute at begin node=\color{black}$\vdots$
						  	},
						  	inputneuron missing/.style={
						  		draw=none, 
						  		scale=2,
						  		text height=0.333cm,
						  		execute at begin node=\color{black}$\vdots$
							},
						}
						
						\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
							% print input neurons
							\foreach \m/\l [count=\y] in {1,2,3,missing,4}
								\node [every inputneuron/.try, inputneuron \m/.try] (input-\m) at (0,2.5-\y) {};
							
							% print hidden neurons
							\foreach \m [count=\y] in {1,missing,2}
								\node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2-\y*1.25) {};
							
							% print output neurons
							\foreach \m [count=\y] in {1,missing,2}
								\node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,1.5-\y) {};
							
							% x_n
							\foreach \l [count=\i] in {1,2,3,n}
								\draw [<-] (input-\i) -- ++(-1,0)
									node [above, midway] {$x_\l$};
							
							% h_n
							\foreach \l [count=\i] in {1,n}
								\node [above] at (hidden-\i.north) {$h_\l$};
							
							% ^x_n
							\foreach \l [count=\i] in {1,n}
							  \draw [->] (output-\i) -- ++(1,0)
							    node [above, midway] {$\hat{x}_\l$};
							
							% lines between input and hidden layer
							\foreach \i in {1,...,4}
							  \foreach \j in {1,...,2}
							    \draw [->] (input-\i) -- (hidden-\j);
							
							% lines between hidden and output layer
							\foreach \i in {1,...,2}
							  \foreach \j in {1,...,2}
							    \draw [->] (hidden-\i) -- (output-\j);
							
							% print labels
							\foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
							  \node [align=center, above] at (\x*2,2) {\l \\ layer};
						\end{tikzpicture}
					}
					\caption{The construction of a simple neural network.}
				\end{figure}
	
				\de{Ein neuronales Netzwerk ist in der Lage komplexe Eingaben zu klassifizieren. Doch wie genau kann man sich das vorstellen? Betrachten wir eine einfache Klassifikationsfunktion, wobei \(\bar{x}\) die Koordinaten der einzelnen Klassenpunkte darstellen und \(\bar{w}\) und \(b\) lernbare Parameter sind:}
				\en{A neural network is able to classify complex inputs. But how exactly can this be imagined? Let us consider a simple classification function, where \(\bar{x}\) represent the coordinates of the individual class points and \(\bar{w}\) and \(b\) are learnable parameters:}
	
				\begin{equation}
					f(\bar{x}, \bar{\omega}, b) = sgn(\bar{\omega}^\intercal \cdot \bar{x} + b)
				\end{equation}

				\de{Mit dieser linearen Funktion lässt sich das nachfolgende Problem leicht klassifizieren:}
				\en{With this linear function the following problem can be easily classified:}

				\begin{figure}[H]
					\begin{center}
						\scalebox{0.5}{\input{images/pgf/linear_classifier.pgf}}
					\end{center}
					\caption{Simple class shattering}
					\label{fig:evaluation_simple_class_shattering}
				\end{figure}
	
				\de{Die Funktion entspricht einem neuronalen Netz ohne verdeckte Schicht und enthält nur eine Eingangs- und eine Ausgangsschicht mit einem künstlichem Neuron ohne Aktivierungsfunktion. Die Dimension, die diese Funktion trennen kann, ist 2 und wird als VC-Dimension\footnote{Vapnik–Chervonenkis dimension: \url{https://en.wikipedia.org/wiki/Convolutional_neural_network}}. Diese Funktion kann genau 2 Klassen trennen. Aber was ist mit nichtlinearen Problemen? In diesem Fall schauen wir uns die folgenden Klassifikationen an:}
				\en{The function corresponds to a neural network without a hidden layer and contains only one input and one output layer with one artificial neuron without activation function. The dimension that this function can separate is 2 and is called VC dimension\footnote{Vapnik–Chervonenkis dimension: \url{https://en.wikipedia.org/wiki/Convolutional_neural_network}}. This function can separate exactly 2 classes. But what about nonlinear problems? In this case let's look at the following classifications:}
				
				\begin{figure}[H]
					\begin{center}
						\scalebox{1.0}{\includetex{classification}}
					\end{center}
					\caption{Linear vs. nonlinear classification}
					\label{fig:overview_linear_nonlinear_classification}
				\end{figure}
	
				\de{Für das zweite Problem kann man die Funktion noch anpassen. Für das dritte nichtlineare Problem ist der Klassifikationsraum nicht mehr ausreichend und erfordert einen anderen Algorithmus. Und hier kommen die neuronalen Netze ins Spiel. Ein Tool, um das Trennen von Daten zu visualisieren und die Funktionsweise der einzelnen Layer zu testen ist \url{https://playground.tensorflow.org}\footnote{Neural Network Right Here in Your Browser: \url{https://playground.tensorflow.org}}. Ein nichtlineares Problem kann im einfachsten Fall schon mit einer hinzugefügten versteckten Schicht mit drei weiteren Neuronen gelöst werden:}
				\en{For the second problem one can still adjust the function. For the third nonlinear problem the classification space is no longer sufficient and requires a different algorithm. And this is where the neural networks come into play. A tool to visualize the separation of data and to test the functionality of the individual layers is \url{https://playground.tensorflow.org}\footnote{Neural Network Right Here in Your Browser: \url{https://playground.tensorflow.org}}. In the simplest case, a nonlinear problem can be solved by adding a hidden layer with three additional neurons:}

				\begin{figure}[H]
					\centering
					\includegraphics[width=1.0\textwidth]{images/simple_neuronal_network}
					\caption[Simple neuronal network with one hidden layer]{Simple neuronal network with one hidden layer\footnotemark}
					\label{fig:beispiel4}
				\end{figure}
				\footnotetext{Source: \url{http://playground.tensorflow.org/}}
		
			\subsubsection{Convolutional neuronal network}
			\label{sec:section_convolutional_neuronal_network}
				\de{Ein neuronales Netzwerk verarbeitet einen Vektor und gibt einen neuen Vektor zurück. Das Problem bei Eingabedaten wie Bildern ist, dass sie auf den ersten Blick nicht erfolgreich als Vektor beschrieben werden können, um mit einem normalem neuronalem Netzwerk trainiert werden zu können. Man braucht einen Algorithmus, welcher auch matrizenähnliche Eingaben verarbeiten kann und in der Lage ist Muster zu erkennen. Dabei wurde in der Vergangenheit das Prinzip der Convolutional Layer entwickelt. Ein Convolutional Layer nimmt ein Matrix Eingang entgegen, transformiert diese und gibt wie auch bei den künstlichen Neuronen einen Ausgangswert zurück (in diesem Fall eine weitere Matrix). Dieser Ausgangswert wird danach an die nächste Schicht weitergegeben. Dabei beinhaltet ein Convolutional Layer eine Menge \(n\) an quadratischen Matrizen (meist 3x3 oder 5x5 Matrizen). Diese Matrizen werden Filter genannt. Jeder Filter\footnote{Filter, welche z.B. Kanten, Ecken, Quadrate, etc. erkennen können und in tieferen Layern Dinge wie Augen, Ohren, Haare, etc.} wird nun jeweils von links oben bis rechts unten über die Pixel des Bildes mittels Skalarprodukt miteinander verrechnet, wobei ein neues Bild entsteht. Die soganannte Feature Map. Bei einer Anzahl von \(n\) Filtern entstehen am Ende \(n\) Feature Maps und heben die in den Filtern definierten Merkmale jeweils im neu errechneten Bild hervor. Dieser Vorgang wird auch Faltung genannt\autocite{deeplizard2017CNNExplained}.}
				\en{A neural network processes a vector and returns a new vector. The problem with input data such as images is that at first view they cannot be successfully described as vectors to be trained with a normal neural network. One needs an algorithm that can handle matrix-like inputs and that is able to recognize patterns. In the past the principle of the convolutional layer was developed. A convolutional layer receives a matrix input, transforms it and returns an output value (in this case another matrix). This output value is then passed on to the next layer. A convolutional layer contains a set \(n\) of square matrices (usually 3x3 or 5x5 matrices). These matrices are called filters. Each filter\footnote{Filters, which can recognize edges, corners, squares, etc. and in deeper layers things like eyes, ears, hair, etc.} is now calculated from top left to bottom right over the pixels of the image using a scalar product, which creates a new image. The so-called Feature Map. With a number of \(n\) filters, \(n\) feature maps are created at the end and highlight the features defined in the filters in the newly calculated image. This process is also called convolution\autocite{deeplizard2017CNNExplained}.}

				\de{Neuronale Netze, welche Gebrauch von Convolutional Layern machen, werden Convolutional Neuronal Networks genannt (kurz CNN) und haben einen entscheidenden Beitrag zum Fortschritt der Bildklassifizierung und auch in anderen Bereichen wie Spracherkennung geleistet. Neben den Convolutional Layern existieren in einem Convolutional Neuronal Network weitere spezielle Layer, welche sich von normalen Neuronalen Netzen unterscheiden: Z.B. die Pooling Layer. In einem Pooling Layer werden überflüssige Informationen verworfen und die Featuremaps verkleinert. Dieser Vorgang verringert den Speicherbedarf und erhöht Berechnungsgeschwindigkeit. Die Convolutional Layer und die Pooling Layer wechseln sich in aller Regel jeweils ab, bis am Ende statt einer \(n x n\) Matrix des Eingangsbildes ein großer Vektor entsteht, welcher von einem normalen neuronalem Netzwerk weiterverarbeitet werden kann und schlußendlich in dem schon beschriebenen One Hot Vektor endet.}
				\en{Neural networks that make use of convolutional layers are called convolutional neural networks (in short CNN) and have made a decisive contribution to the progress of image classification and also in other areas like speech recognition. In addition to the Convolutional Layers, a Convolutional Neural Network has other special layers that differ from normal neural networks: For example the Pooling Layer. In a pooling layer, unnecessary information is discarded and feature maps are reduced in size. This process reduces memory requirements and increases calculation speed. The convolutional layer and the pooling layer usually alternate until a large vector is created at the end instead of a \(n x n\) matrix of the input image, which can be further processed by a normal neural network and finally ends in the already described one hot vector.}

				\hl{Idea?} \url{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}

				\begin{figure}[H]
					\centering
					\begin{tikzpicture}
						\tikzmath{
							\inputImageWidth = 1.0;
							\layerOneDistance = 0.22;
							\layerOnePosition = 1.95;
							\layerOneWidth = 1.0;
							\layerTwoDistance = 0.12;
							\layerTwoPosition = 4.40;
							\layerTwoWidth = 0.6;
							\layerThreeDistance = 0.12;
							\layerThreePosition = 6.50;
							\layerThreeWidth = 0.6;
							\layerFourDistance = 0.052;
							\layerFourPosition = 8.5;
							\layerFourWidth = 0.36;
						}

						\node[anchor=south west,inner sep=0,framed] (burger) at (0,0)
							{\includegraphics[width=6.5ex]{images/data/burger/burger28-quad.png}};

						% input image
						\node at (0.5,-1){\begin{tabular}{c}input image\end{tabular}};
						%\draw[opacity=1, draw=black]
						%	(0 * \inputImageWidth, 0 * \inputImageWidth) --
						%	(1 * \inputImageWidth, 0 * \inputImageWidth) --
						%	(1 * \inputImageWidth, 1 * \inputImageWidth) --
						%	(0 * \inputImageWidth, 1 * \inputImageWidth) --
						%	(0 * \inputImageWidth, 0 * \inputImageWidth);
						
						% layer 1
						\node at (3.0, 3.0){\begin{tabular}{c}convolutional layer\\layer $l = 1$\end{tabular}};
						\foreach \i in {5,...,0}
							\draw[fill=gray, opacity=0.8, draw=black]
								(\layerOnePosition + 0 * \layerOneWidth + \layerOneDistance*\i, 0 * \layerOneWidth + \layerOneDistance*\i) --
								(\layerOnePosition + 1 * \layerOneWidth + \layerOneDistance*\i, 0 * \layerOneWidth + \layerOneDistance*\i) --
								(\layerOnePosition + 1 * \layerOneWidth + \layerOneDistance*\i, 1 * \layerOneWidth + \layerOneDistance*\i) --
								(\layerOnePosition + 0 * \layerOneWidth + \layerOneDistance*\i, 1 * \layerOneWidth + \layerOneDistance*\i) --
								(\layerOnePosition + 0 * \layerOneWidth + \layerOneDistance*\i, 0 * \layerOneWidth + \layerOneDistance*\i);

						% layer 2
						\node at (5.0,-1){\begin{tabular}{c}pooling layer\\layer $l = 2$\end{tabular}};
						\foreach \i in {5,...,0}
							\draw[fill=gray, opacity=0.8, draw=black]
								(\layerTwoPosition + 0 * \layerTwoWidth + \layerTwoDistance*\i, 0 * \layerTwoWidth + \layerTwoDistance*\i) --
								(\layerTwoPosition + 1 * \layerTwoWidth + \layerTwoDistance*\i, 0 * \layerTwoWidth + \layerTwoDistance*\i) --
								(\layerTwoPosition + 1 * \layerTwoWidth + \layerTwoDistance*\i, 1 * \layerTwoWidth + \layerTwoDistance*\i) --
								(\layerTwoPosition + 0 * \layerTwoWidth + \layerTwoDistance*\i, 1 * \layerTwoWidth + \layerTwoDistance*\i) --
								(\layerTwoPosition + 0 * \layerTwoWidth + \layerTwoDistance*\i, 0 * \layerTwoWidth + \layerTwoDistance*\i);
						
						%layer 3
						\node at (7.5,3.0){\begin{tabular}{c}convolutional layer\\layer $l = 3$\end{tabular}};
						\foreach \i in {11,...,0}
							\draw[fill=gray, opacity=0.8, draw=black]
								(\layerThreePosition + 0 * \layerThreeWidth + \layerThreeDistance*\i, 0 * \layerThreeWidth + \layerThreeDistance*\i) --
								(\layerThreePosition + 1 * \layerThreeWidth + \layerThreeDistance*\i, 0 * \layerThreeWidth + \layerThreeDistance*\i) --
								(\layerThreePosition + 1 * \layerThreeWidth + \layerThreeDistance*\i, 1 * \layerThreeWidth + \layerThreeDistance*\i) --
								(\layerThreePosition + 0 * \layerThreeWidth + \layerThreeDistance*\i, 1 * \layerThreeWidth + \layerThreeDistance*\i) --
								(\layerThreePosition + 0 * \layerThreeWidth + \layerThreeDistance*\i, 0 * \layerThreeWidth + \layerThreeDistance*\i);
						
						% layer 4
						\node at (9.0,-1){\begin{tabular}{c}pooling layer\\layer $l = 4$\end{tabular}};
						\foreach \i in {11,...,0}
							\draw[fill=gray, opacity=0.8, draw=black]
								(\layerFourPosition + 0 * \layerFourWidth + \layerFourDistance*\i, 0 * \layerFourWidth + \layerFourDistance*\i) --
								(\layerFourPosition + 1 * \layerFourWidth + \layerFourDistance*\i, 0 * \layerFourWidth + \layerFourDistance*\i) --
								(\layerFourPosition + 1 * \layerFourWidth + \layerFourDistance*\i, 1 * \layerFourWidth + \layerFourDistance*\i) --
								(\layerFourPosition + 0 * \layerFourWidth + \layerFourDistance*\i, 1 * \layerFourWidth + \layerFourDistance*\i) --
								(\layerFourPosition + 0 * \layerFourWidth + \layerFourDistance*\i, 0 * \layerFourWidth + \layerFourDistance*\i);
						
						% layer 5
						\node at (12,3.0){\begin{tabular}{c}fully connected layer\\layer $l = 5$\end{tabular}};
						\draw[fill=teal,draw=black,opacity=0.8]
							(10.5,0) --
							(11,0) --
							(12.75,1.75) --
							(12.25,1.75) --
							(10.5,0);
						
						% layer 6
						\node at (13,-1){\begin{tabular}{c}fully connected layer\\output layer $l = 6$\end{tabular}};
						\draw[fill=teal,draw=black,opacity=0.8]
							(12.5,0.5) --
							(13,0.5) --
							(13.65,1.15) --
							(13.15,1.15) --
							(12.5,0.5);
					\end{tikzpicture}
					\caption[Architecture of a traditional convolutional neural network.]{Architecture of a traditional convolutional neural network.}
					\label{fig:traditional-convolutional-network}
				\end{figure}

				\de{Ein großer Vorteil von Convolutional neuronal networks soll nicht unerwähnt bleiben: Sie benötigen relativ wenig Vorverarbeitung im Vergleich zu anderen Bildklassifikationsalgorithmen. Dies bedeutet, dass das Netzwerk eigenständig die Filter lernt, die in herkömmlichen Algorithmen normalerweise von Hand entwickelt werden, wenn es mit ausreichender Schulung trainiert wird. Diese Eigenschaft dieser Netzwerke ist von großem Vorteil, da sie automatisiert durchgeführt werden können und sich bei Änderungen der Eingabedaten selbstständig ändern und keinem menschlichem Eingriff bedarf.}
				\en{A big advantage of convolutional neural networks should not remain unmentioned: They require relatively little preprocessing compared to other image classification algorithms. This means that the network independently learns the filters that are normally developed by hand in conventional algorithms, if trained with adequate training. This property of these networks is a great advantage because they can be automated and change independently when the input data changes and do not require human intervention.}
				
				\begin{figure}[H]
					\centering
					\begin{tabular}{ ccccccc }
						image matrix & & filter matrix & & output matrix & & pooling \\
					
						\begin{tabular}{|c|c|c|c|c|c|}
							\hline
							\tikzmark{tl11}0 & 1 & 1 & 0 & 1 & 1\\
							\hline
							1 & 1 & 1 & 0 & 1 & 0\\
							\hline
							0 & 1 & \tikzmark{tl21}0\tikzmark{br11} & 1 & 0 & 1\\
							\hline
							0 & 0 & 1 & 0 & 0 & 0\\
							\hline
							0 & 0 & 0 & 0 & 0\tikzmark{br21} & 1\\
							\hline
							0 & 0 & 1 & 0 & 0 & 0\\
							\hline
						\end{tabular} &
					
						\circled{$\times$} &
						\begin{tabular}{|c|c|c|}
							\hline
							1 & 1 & 1\\
							\hline
							1 & 1 & 0\\
							\hline
							0 & 0 & 1\\
							\hline
						\end{tabular} &
					
						\LARGE{$\rightarrow$} &
					
						\begin{tabular}{|c|c|c|c|}
							\hline
							\tikzmark{tl12}4\tikzmark{br12} & 5 & \tikzmark{tl31}3 & 4\\
							\hline
							5 & 3 & 3 & 2\tikzmark{br31}\\
							\hline
							1 & 3 & \tikzmark{tl22}2\tikzmark{br22} & 3\\
							\hline
							2 & 1 & 1 & 0\\
							\hline
						\end{tabular} &
						
						\LARGE{$\rightarrow$} &
						\begin{tabular}{|c|c|c|}
							\hline
							5 & \tikzmark{tl32}4\tikzmark{br32}\\
							\hline
							3 & 3\\
							\hline
						\end{tabular}
					\end{tabular}
					
					\DrawBox[thick, red]{tl11}{br11}
					\DrawBox[thick, red]{tl12}{br12}
					\DrawBox[thick, blue]{tl21}{br21}
					\DrawBox[thick, blue]{tl22}{br22}
					
					\DrawBox[thick, black!30!green, densely dotted, fill=green!50!white, fill opacity=0.2]{tl31}{br31}
					\DrawBox[thick, black!30!green, densely dotted, fill=green!50!white, fill opacity=0.2]{tl32}{br32}
					
					\caption[Simple convolution and pooling.]{Simple convolution and pooling}
				\end{figure}
	
			\subsubsection{Transfer learning}
			\label{sec:section_transfer_learning}
				\de{Convolutional neuronal networks sind großartig und haben einen entscheidenden Beitrag zur Klassifizierung von Bilder beigetragen. Mit der Gründung der Forschungsdatenbank ImageNet im Jahre 2006 werden jährliche Wettbewerbe veranstaltet, um entwickelte Neuronale Netzwerke miteinander zu vergleichen. ImageNet ist eine Bilderdatenbank mit mehr als 14 Millionen Bildern. Ein CNN namens AlexNet im Jahr 2012 einen Top-5 Fehler von 15,3\% und erhöht sich aktuell stetig jedes Jahr. Aber die Architektur von einem CNN hat ein Problem. Alle Convolutional Layer sind vom Beginn an zufällig initialisiert und enthalten noch keine Muster. Damit sie zuverlässig funktioniert, muss sie mit vielen Bildern trainiert werden. Würde man vom Scratch an ein CNN selbst entwickeln und verwenden, so müssen alle Convolutional Layer auch vorab trainiert werden.}
				\en{Convolutional neural networks are great and have made a significant contribution to the classification of images. With the foundation of the research database ImageNet in 2006, annual competitions are organized to compare developed neural networks. ImageNet is an image database with more than 14 million images. A CNN called AlexNet in 2012 got a top-5 error of 15.3\% and is currently increasing steadily every year. But the architecture of a CNN has a problem. All convolutional layers are randomly initialized from the beginning and do not yet contain any patterns. For it to work reliably, it needs to be trained with many images. If one would develop and use a CNN from scratch, all convolutional layers have to be trained in advance.}

				\de{Die Convolutional Layer extrahieren Features wie Kanten, Quadrate, Kreise, etc. Diese sind so gut wie in jedem Bild vorhanden und es stellt sich die Frage, ob man diese nicht wiederverwenden kann, um den Trainingsaufwand zu verringern. Die Idee beim Transfer Learning ist es ein schon vortrainiertes CNN zu verwenden und lediglich das neuronale Netzwerk am Ende des Convolutional neuronal networks an die eigene Problemstellung anzupassen:}
				\en{The convolutional layers extract features such as edges, squares, circles, etc. These are present in almost every image and the question arises whether one can reuse them to reduce the training effort. The idea of Transfer Learning is to use an already pre-trained CNN and just adapt the neural network at the end of the Convolutional neural network to the own problem:}

				\begin{figure}[H]
					\centering
					\begin{tikzpicture}
						\tikzmath{
							\inputImageWidth = 1.0;
							\layerOneDistance = 0.22;
							\layerOnePosition = 1.95;
							\layerOneWidth = 1.0;
							\layerTwoDistance = 0.12;
							\layerTwoPosition = 4.40;
							\layerTwoWidth = 0.6;
							\layerThreeDistance = 0.12;
							\layerThreePosition = 6.50;
							\layerThreeWidth = 0.6;
							\layerFourDistance = 0.052;
							\layerFourPosition = 8.5;
							\layerFourWidth = 0.36;
						}

						% placeholder
						\draw[fill=white, opacity=0, draw=white]
							(0 * \inputImageWidth, 0 * \inputImageWidth + .5) --
							(1 * \inputImageWidth, 0 * \inputImageWidth + .5) --
							(1 * \inputImageWidth, 1 * \inputImageWidth + .5) --
							(0 * \inputImageWidth, 1 * \inputImageWidth + .5) --
							(0 * \inputImageWidth, 0 * \inputImageWidth + .5);
						\draw[fill=white, opacity=0, draw=white]
							(0 * \inputImageWidth, 0 * \inputImageWidth - .5) --
							(1 * \inputImageWidth, 0 * \inputImageWidth - .5) --
							(1 * \inputImageWidth, 1 * \inputImageWidth - .5) --
							(0 * \inputImageWidth, 1 * \inputImageWidth - .5) --
							(0 * \inputImageWidth, 0 * \inputImageWidth - .5);

						%draw image
						%\node at (0.5,-1){\begin{tabular}{c}input image\end{tabular}};
						\node[anchor=south west,inner sep=0] (burger) at (0,0)
							{\includegraphics[width=6.5ex]{images/data/burger/burger28-quad.png}};

						% input image
						%\draw[fill=white, opacity=1, draw=black]
						%	(0 * \inputImageWidth, 0 * \inputImageWidth) --
						%	(1 * \inputImageWidth, 0 * \inputImageWidth) --
						%	(1 * \inputImageWidth, 1 * \inputImageWidth) --
						%	(0 * \inputImageWidth, 1 * \inputImageWidth) --
						%	(0 * \inputImageWidth, 0 * \inputImageWidth);
						
						% layer 1
						\foreach \i in {5,...,0}
							\draw[fill=gray, opacity=0.8, draw=black]
								(\layerOnePosition + 0 * \layerOneWidth + \layerOneDistance*\i, 0 * \layerOneWidth + \layerOneDistance*\i) --
								(\layerOnePosition + 1 * \layerOneWidth + \layerOneDistance*\i, 0 * \layerOneWidth + \layerOneDistance*\i) --
								(\layerOnePosition + 1 * \layerOneWidth + \layerOneDistance*\i, 1 * \layerOneWidth + \layerOneDistance*\i) --
								(\layerOnePosition + 0 * \layerOneWidth + \layerOneDistance*\i, 1 * \layerOneWidth + \layerOneDistance*\i) --
								(\layerOnePosition + 0 * \layerOneWidth + \layerOneDistance*\i, 0 * \layerOneWidth + \layerOneDistance*\i);

						% layer 2
						\foreach \i in {5,...,0}
							\draw[fill=gray, opacity=0.8, draw=black]
								(\layerTwoPosition + 0 * \layerTwoWidth + \layerTwoDistance*\i, 0 * \layerTwoWidth + \layerTwoDistance*\i) --
								(\layerTwoPosition + 1 * \layerTwoWidth + \layerTwoDistance*\i, 0 * \layerTwoWidth + \layerTwoDistance*\i) --
								(\layerTwoPosition + 1 * \layerTwoWidth + \layerTwoDistance*\i, 1 * \layerTwoWidth + \layerTwoDistance*\i) --
								(\layerTwoPosition + 0 * \layerTwoWidth + \layerTwoDistance*\i, 1 * \layerTwoWidth + \layerTwoDistance*\i) --
								(\layerTwoPosition + 0 * \layerTwoWidth + \layerTwoDistance*\i, 0 * \layerTwoWidth + \layerTwoDistance*\i);
						
						%layer 3
						\foreach \i in {11,...,0}
							\draw[fill=gray, opacity=0.8, draw=black]
								(\layerThreePosition + 0 * \layerThreeWidth + \layerThreeDistance*\i, 0 * \layerThreeWidth + \layerThreeDistance*\i) --
								(\layerThreePosition + 1 * \layerThreeWidth + \layerThreeDistance*\i, 0 * \layerThreeWidth + \layerThreeDistance*\i) --
								(\layerThreePosition + 1 * \layerThreeWidth + \layerThreeDistance*\i, 1 * \layerThreeWidth + \layerThreeDistance*\i) --
								(\layerThreePosition + 0 * \layerThreeWidth + \layerThreeDistance*\i, 1 * \layerThreeWidth + \layerThreeDistance*\i) --
								(\layerThreePosition + 0 * \layerThreeWidth + \layerThreeDistance*\i, 0 * \layerThreeWidth + \layerThreeDistance*\i);
						
						% layer 4
						\foreach \i in {11,...,0}
							\draw[fill=gray, opacity=0.8, draw=black]
								(\layerFourPosition + 0 * \layerFourWidth + \layerFourDistance*\i, 0 * \layerFourWidth + \layerFourDistance*\i) --
								(\layerFourPosition + 1 * \layerFourWidth + \layerFourDistance*\i, 0 * \layerFourWidth + \layerFourDistance*\i) --
								(\layerFourPosition + 1 * \layerFourWidth + \layerFourDistance*\i, 1 * \layerFourWidth + \layerFourDistance*\i) --
								(\layerFourPosition + 0 * \layerFourWidth + \layerFourDistance*\i, 1 * \layerFourWidth + \layerFourDistance*\i) --
								(\layerFourPosition + 0 * \layerFourWidth + \layerFourDistance*\i, 0 * \layerFourWidth + \layerFourDistance*\i);
						
						% layer 5
						\draw[fill=red,draw=black,opacity=0.8]
							(10.5,0) --
							(11,0) --
							(12.75,1.75) --
							(12.25,1.75) --
							(10.5,0);
						
						% layer 6
						\draw[fill=red,draw=black,opacity=0.8]
							(12.5,0.5) --
							(13,0.5) --
							(13.65,1.15) --
							(13.15,1.15) --
							(12.5,0.5);
					\end{tikzpicture}
					\caption[The green area (the neural network) was replaced by a network (red) adapted to the new problem.]{The green area (the neural network) was replaced by a network (red) adapted to the new problem (see \fullref{fig:traditional-convolutional-network}).}
					\label{fig:transfer-learning-example}
				\end{figure}

				\de{Welchen Vorteil ein vortrainiertes Netzwerk hat, kann man in dieser Arbeit im Kapitel ``\nameref{sec:section_use_of_the_transfer_learning_approach}" einsehen.}
				\en{The advantage of a pre-trained network can be seen in the chapter ``\nameref{sec:section_use_of_the_transfer_learning_approach}" of this thesis.}
			
			\subsubsection{Overview of current and known convolutional neural networks}
				\de{Zu guter Letzt folgen hier noch ein paar aktuelle und bekannte Convolutional Neuronal networks. Sie unterscheiden sich hauptsächlich in folgenden Metriken, wobei in Kombination jedes Netzwerk seine Vor- und Nachteile besitzt:}
				\en{Last but not least, here are a few current and well-known convolutional neural networks. They differ mainly in the following metrics, whereby in combination each network has its advantages and disadvantages:}

				\begin{itemize}
					\item the top-1 accuracy (based on the ImageNet image dataset)
					\item the computing operations which are required for a single forward pass (G-Ops)
					\item the model size (for comparison: the model size of InceptionV3 is about 180 MB)
				\end{itemize}
		
				\begin{figure}[H]
					\centering
					\includegraphics[width=1.0\textwidth]{images/tl_models}
					\caption[Overview of current and known convolutional neural networks.]{Overview of current and known convolutional neural networks.\footnotemark}
					\label{fig:comparison_cnn}
				\end{figure}
				\footnotetext{Source: \url{https://towardsdatascience.com/neural-network-architectures-156e5bad51ba}}

		\subsection{Further definitions}
		\label{sec:section_further_definitions}
			\subsubsection{Learning epoch}
			\label{sec:section_learning_epoch}
				\de{Unter einer Lernepoche versteht man den einmaligen Trainingsdurchlauf mit allen Trainingsdatensätzen. Meist reicht ein Trainingsdurchlauf nicht aus, weshalb weitere Lernepochen im Anschluß erfolgen. Sobald mit weiteren Lernepochen die Leistungsfähigkeit des Netzwerkes nicht weiter steigt, beendet man das Training.}
				\en{A learning epoch is understood to be the one time training run with all training data sets. Usually one training run is not sufficient, which is why further learning epochs follow. As soon as the performance of the network does not increase anymore with further epochs, the training is terminated.}
			\subsubsection{Learning rate}
			\label{sec:section_learning_rate}
				\de{Die Lernrate ist ein Abstimmungsparameter in einem Optimierungsalgorithmus. Dieser sagt aus, wie groß die Schrittweite bei jeder Iteration ist, mit der sich die Parameter auf das Minimum einer Verlustfunktion zubewegen. Sie sollte nicht zu hoch (Minimum wird nicht gefunden) und nicht zu klein sein (sehr langsames Lernen). Die Lernrate wird oft mit dem Zeichen \(\eta\) oder \(\alpha\) bezeichnet.}
				\en{The learning rate is a tuning parameter in an optimization algorithm. It tells us how large the step size is for each iteration with which the parameters move closer to the minimum of a loss function. It should not be too high (minimum is not found) and not too small (very slow learning). The learning rate is often indicated by the character \(\eta\) or \(\alpha\).}
			\subsubsection{Batch size}
			\label{sec:section_batch_size}
				\de{Die Batch Size definiert die Anzahl der Bilder, die gleichzeitig über das Netzwerk übertragen werden, bevor die Parameter im Netzwerk angepasst werden. Normalerweise ist es pro Epoche vorgesehen alle Trainingsdaten mit einem Vorgang zu trainieren. Dies ist gerade im Bereich der Bildklassifizierung aufgrund des begrenzten Speichers meist nicht möglich, weshalb man eine Epoche in viele kleine Batches aufteilt. Der Vorteil besteht darin, dass man nur die Daten im Speicher halten muss, die für den aktuellen Batch nötig sind. Der Nachteil ist der erhöhte Rechenaufwand und eine ungenaue Schätzung des Gradienten, da sich die Änderung nur auf den aktuellen Batch und nicht auf alle Bilder bezieht\footnote{``What is batch size in neural network?'', itdxer on stackexchange.com, February 9, 2020, \url{https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network}}.}
				\en{The batch size defines the number of images that are transmitted simultaneously over the network before the parameters in the network are customized. Normally it is intended to train all training data per epoch with one process. Especially in the area of image classification this is usually not possible due to the limited memory, so an epoch is divided into many small batches. The advantage is that only the data necessary for the current batch needs to be kept in memory. The disadvantage is the increased computing effort and an inaccurate estimation of the gradient, because the change only applies to the current batch and not to all images\footnote{``What is batch size in neural network?'', itdxer on stackexchange.com, February 9, 2020, \url{https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network}}.}

			\subsubsection{Data augmentation}
			\label{sec:section_data_augmentation}
				\de{Als Data Augmentation bezeichnet man die künstliche Vergrößerung eines Datensets. Dies wird vorrangig dann angewendet, wenn man nur wenig Daten zur Verfügung hat. Dabei werden bestehende Bilder gedreht, gespiegelt, Farbkorrekturen vorgenommen, ausgeschnitten, etc. Die Data Augmentation kann die Modellgenauigkeit beim Training verbessern.}
				\en{Data augmentation is the artificial augmentation of a data set. This is primarily used when only little data is available. Existing images are rotated, mirrored, color adjusted, cropped, etc. Data Augmentation can improve model accuracy during training. \hl{Is there an example that proves this quickly?}}
					
					
					
	% -------------------- %
	% Insufficient amount of data %
	% -------------------- %
	\section{Insufficient amount of data}
	\label{sec:section_insufficient_amount_of_data}
		\de{Gibt man einem Menschen einen Donut und erkärt ihm, dass es ein Donut ist, so ist dieser nach einigem Wiederholen selbstständig in der Lage diesen Donut in der Zukunft zu klassifizieren. Bei Machine Learning ist diese Problematik etwas komplexer. Wie bei den meisten machinellen Lernverfahren benötigt man eine große Menge an Daten. Wieviel ist nicht richtig belegt. Gerade wenn man es mit vielen vorherzusagenden Klassen zu tun hat, erhöht sich die Datenmenge erfahrungsgemäß. Einige Meinungen in Foren und Blogartikeln besagen (Hypothese), dass es mindestens 1000 Bilder pro Klasse sein müssen.\footnote{``Deep Learning for Image Classification with Less Data'', https://towardsdatascience.com, February 2, 2020, \url{https://towardsdatascience.com/deep-learning-for-image-classification-with-less-data-90e5df0a7b8e}}\textsuperscript{,}\footnote{``How many images do you need to train a neural network?'', https://petewarden.com, February 2, 2020, \url{https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/}}\textsuperscript{,}\footnote{``What is the minimum sample size required to train a Deep Learning model - CNN?'', https://www.researchgate.net, February 2, 2020, \url{https://www.researchgate.net/post/What_is_the_minimum_sample_size_required_to_train_a_Deep_Learning_model-CNN}}\textsuperscript{,}\autocite{krizhevsky2012imagenet}.}
		\en{If one gives a person a doughnut and explain to him that it is a doughnut, then after some repetition he is able to classify this doughnut in the future. With Machine Learning this problem is a bit more complex. As with most machine learning methods, a large amount of data is required. How much is not properly documented. Especially when one is dealing with many classes to be predicted, experience shows that the amount of data increases. Some opinions in forums and blog articles say (hypothesis) that there must be at least 1000 pictures per class.\footnote{``Deep Learning for Image Classification with Less Data'', https://towardsdatascience.com, February 2, 2020, \url{https://towardsdatascience.com/deep-learning-for-image-classification-with-less-data-90e5df0a7b8e}}\textsuperscript{,}\footnote{``How many images do you need to train a neural network?'', https://petewarden.com, February 2, 2020, \url{https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/}}\textsuperscript{,}\footnote{``What is the minimum sample size required to train a Deep Learning model - CNN?'', https://www.researchgate.net, February 2, 2020, \url{https://www.researchgate.net/post/What_is_the_minimum_sample_size_required_to_train_a_Deep_Learning_model-CNN}}\textsuperscript{,}\autocite{krizhevsky2012imagenet}}

		\de{Je nach Anzahl von zu trainierenden Klassen gelangt man somit schnell zu einem benötigten Datenset, welches aus mehrere Gigabyte an Daten besteht. Mittels Transfer Learning ist es möglich diese Zahl nochmals etwas zu reduzieren, jedoch bleibt das Problem der vielen Daten bestehen. Ein Paper von Microsoft aus dem Jahre 2001 zeigte zur damaligen Zeit, dass einfache Algorithmen mit genügend Daten ähnliche Ergebnisse lieferten, wie komplexe Algorithmen auf Basis weniger Daten. Dabei bezogen sich die Forscher auf Daten, welche Sprachkonstrukte klassifizieren sollten:}
		\en{Depending on the number of classes to be trained, one will quickly arrive at the required data set, which consists of several gigabytes of data. With Transfer Learning it is possible to reduce this number a little bit, but the problem of the large amount of data remains. A paper  from Microsoft in 2001 showed at that time that simple algorithms with enough data gave similar results as complex algorithms based on less data. The researchers referred to data which should classify language constructs:}
		
		\blockquote{\textit{We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets.}}\autocite{banko2001scaling}
		
		\de{In einem anderen Artikel nur wenige Jahre später wird dieses Thema ebenfalls aufgegriffen. Dabei bezog man sich auf Daten, welche von Texten lernen und man meist nur kleine oder mittelgroße Datensätze zur Verfügung hat. Um die Effizienz auch in diesem Fall zu verbessern, ist es eine gute Idee die Algorithmen und Methoden zu verbessern: \hl{Beispiele!}}
		\en{Another article only a few years later also addresses this issue. This referred to data that learn from texts and that usually only small or medium sized data sets are available. To improve the efficiency also in this case, it is a good idea to improve the algorithms and methods: \hl{Examples!}}

		\blockquote{\textit{...\autocite{halevy2009unreasonable}.}}



	% -------------------- %
	% Related work %
	% -------------------- %
	\section{Related work}
		\de{Es existieren eine Reihe von Untersuchungen zur Bildklassifikation auf sehr großen Datensätzen.\autocite{deng2010does}\textsuperscript{,}\autocite{sun2014deep}\textsuperscript{,}\autocite{krizhevsky2012imagenet} Meist ist diese riese Anzahl von Klassifikationsklassen und Datenmengen gar nicht erwünscht. Weiterhin gibt es viele Untersuchungen auf kleinen Datensätzen. Was ihnen jedoch fehlt ist die Tatsache die gängisten Abstimmungsparameter in einer Übersicht zu finden. Daten sind teuer in der Beschaffung und meist nicht einfach zu bekommen (siehe Kapitel ``\hyperref[sec:section_balanced_training_data_set]{Balanced training data set}''). Und im Gegensatz dazu, treten Bildklassifikationen in immer mehr Bereichen unseres Lebens auf und finden auch in immer mehr in Unternehmen eine direkte Verwendung, welche bisher keinen Gebrauch davon gemacht haben und sich nun überlegen eigene Implemantionen einzuführen. Da sind z.B. Unternehmen, welche versuchen aufgrund verschiedener Daten Produkte zu klassifizieren. Ist es unter dem Kostenpunkte betrachtet eine gute Idee oder bleibt einem der Schritt zu den Softwaretools von Unternehmensriesen wie Microsoft, Google und Co. nicht verwehrt? Ein Mensch sieht einen Artikel und ordnet ihn ein: Anhand des Textes, der Beschreibungung oder eines Bildes. Manchmal bleibt nur noch ein Bild, weil z.B. Texte nicht richtig gepflegt wurden oder nur kryptische Werte zurückliefern. Auch diese Klassifizierung ist für den Menschen oftmals keine Herausforderung. Er erkennt auch in diesem Fall das Produkt \(X\) aufgrund des noch vorhandenen Bildes.}
		\en{There are a number of studies on image classification on very large datasets.\autocite{deng2010does}\textsuperscript{,}\autocite{sun2014deep}\textsuperscript{,}\autocite{krizhevsky2012imagenet} Most of the time this huge number of classification classes and datasets is not desired. Furthermore, there are many investigations on small data sets. What they are missing is the fact that the most common tuning parameters can be found in one overview. Data is expensive to obtain and usually not easy to get (see chapter ``\hyperref[sec:section_balanced_training_data_set]{Balanced training data set}''). And in contrast, image classifications are appearing in more and more areas of our life and are also being used directly in more and more companies that have not made use of them so far and are now considering introducing their own implementations. There are e.g. companies which try to classify products based on different data. Is it a good idea to implement your own implementation or is the step to the software tools of company giants like Microsoft, Google and Co. unavoidable? A person sees an article and classifies it: From the text, the description or an image. Sometimes only an image remains, because e.g. texts have not been maintained properly or only return cryptic values. Even this classification is often not a challenge for humans. In this case he also recognizes the product \(X\) on the basis of the still existing image.}

		\de{Diese Arbeit beschäftigt sich mit dem Bildklassifizierungsteil. Da es sich hierbei um Unternehmen mit begrenzten Ressourcen handelt, steht nun die Frage im Raum: Mit welchen Mitteln, Techniken und Kniffen ist es möglich dennoch eine erfolgreiche Bildklassifizierung durchzuführen. Müssen die in \fullref{sec:section_insufficient_amount_of_data} erwähnten Bedingungen erfüllt werden oder sind erfolgreiche Klassifizierungen auch schon mit weniger möglich? Kann man durch Anpassung bestimmter Abstimmungsparameter das Optimum aus der Erstellung von Modellen herausholen? Ist z.B. eine Clusteranalyse und die damit verbundene kategorische Untergliederung der Klassifizierung ein erfolgreicher Ansatz? Diese und andere Fragen sollen mit dieser Arbeit geklärt werden. Und ich wage an dieser Stelle zu behaupten, dass es möglich ist auch mit weniger Anforderungen zu einem guten Ziel zu kommen.}
		\en{This thesis deals with the image classification part. Since these are companies with limited resources, the question is: With which tools, techniques and tricks is it possible to carry out a successful image classification. Do the conditions mentioned in \fullref{sec:section_insufficient_amount_of_data} have to be fulfilled or are successful classifications already possible with less? Is it possible to get the most out of model creation by adjusting certain tuning parameters? For example, is a cluster analysis and the associated categorical breakdown of the classification a successful approach? These and other questions are to be clarified with this thesis. And I dare to say at this point that it is possible to reach a good goal even with fewer requirements.}



	% -------------------- %
	% model validation %
	% -------------------- %
	\section{Model validation}
	\label{sec:section_validation_process}
		\de{In diesem Teil der Arbeit werden die Untersuchungen vorgestellt und die entsprechenden Auswertungen dazu. Dabei wurden in den jeweiligen Kapiteln Modelle mit den angegebenen Parametern und Techniken erstellt mit welcher man in der Lage ist Bildklassifikationen durchzuführen. Das Modell muss trainiert werden, wobei eine Entscheidung auf viele variable Parameter wie die Lernrate \(\eta\), die Optimierer, aber auch Dinge wie das CNN Modell gefällt werden muss. Als Basis dient ein Standardsetup, welches im nachfolgendem Kapitel ``\nameref{sec:section_validation_process_preamble}'' erklärt wird. Von diesem Standard-Setup ausgehend werden die variablen Parameter in den entsprechenden Kapiteln jeweils verändert und diskutiert. Das Ziel soll sein die Parameter zu finden, mit welchem die Erkennungsgenauigkeiten der Modelle besonders hoch ausfallen: hohe Genauigkeiten und kleiner Verlust (loss function).}
		\en{In this part of the thesis the investigations and the corresponding evaluations will be presented. For each chapter models with the given parameters and techniques have been created with which image classifications can be performed. The model has to be trained, whereby a decision has to be made on many variable parameters like the learning rate \(\eta\), the optimizers, but also things such as the CNN model. The basis is a standard setup, which is explained in the following chapter ``\nameref{sec:section_validation_process_preamble}''. Starting from this standard setup, the variable parameters are changed and discussed in the corresponding chapters. The goal is to find the parameters with which the recognition accuracy of the models is particularly high: high accuracy and small loss function.}
	
		\subsection{Preamble}
		\label{sec:section_validation_process_preamble}
			\de{Bevor man den Validierungsprozess beginnen kann, muss der verwendete Datensatz entsprechend vorbereitet werden. Bei diesem in dieser Arbeit verwendetem Datensatz handelt es sich um einen Essensdatensatz mit insgesamt 50 Klassen: \texttt{food-50}. Dieser Datensatz enthält 50 Ordner mit den jeweiligen vorhandenen vorklassifizierten Bildern bei einer Größe von 765 MB. Die Ordner müssen in einen Validierungs- und einen Trainingsdatensatz aufgeteilt werden. Als Verhältnis wurden 20\% Validierungs- und 80\% Trainingsdaten gewählt:}
			\en{Before the validation process can be started, the data set used must be prepared properly. The data set used in this thesis is a meal data set with a number of 50 classes: \texttt{food-50}. This data set contains 50 folders with the respective existing pre-classified images with a size of 765 MB. The folders must be divided into a validation record and a training record. The ratio is 20\% validation and 80\% training data:}
			
			\subsubsection*{Training data}
				\begin{itemize}
					\item 11913 images
					\item classified within 50 classes
					\item different number of images per class (unbalanced)
				\end{itemize}
			
			\subsubsection*{Validation data}
				\begin{itemize}
					\item 2953 images
					\item classified within 50 classes
					\item different number of images per class (unbalanced)
				\end{itemize}
				
			\subsubsection*{Default setup}
				\de{Mit Ausnahme der CNN-Modellversuche basierten alle Tests auf den folgenden Parametern (wobei ein Wert der Parameter je nach Kapitel variierte):}
				\en{With the exception of the cnn model tests, all tests were based on the following parameters
			(whereby one value of the parameters varied depending on the chapter):}
		
				\begin{itemize}
					\item \textbf{model:} InceptionV3
					\item \textbf{learning rate:} 0,001 (decreases every 7 epochs to 50\% of the previous value)
					\item \textbf{epochs:} 21 (learning rate from epoch 15 to 21: 0,00025)
					\item \textbf{image size:} 299x299 pixels
					\item \textbf{batch size:} 16
					\item \textbf{drop out:} 50\%
					\item \textbf{cnn weights:} imagenet
					\item \textbf{activation function:} relu
					\item \textbf{optimizer:} sgd with Nesterov
					\item \textbf{momentum:} 0.9 (with decay 0.0)
					\item the entire training and validation set (14866 images - unless otherwise specified)
				\end{itemize}
				
			\de{\noindent Verschiedene Modelle wurden im Kapitel ``\nameref{sec:section_validation_compare_cnn_models}'' mit den gleichen Parametern wie oben ausprobiert:}
			\en{\noindent Different models were tried out in chapter ``\nameref{sec:section_validation_comparison_cnn_models}'' with the same parameters as above:}
		
			\begin{itemize}
				\item DenseNet121
				\item DenseNet201
				\item InceptionResNetV2
				\item InceptionV3
				\item NASNetLarge
				\item ResNet50
				\item VGG19
				\item Xception
			\end{itemize}
			
		\subsection{Working environment and model creation}
			\de{Der gesamte Quellcode für die Umgebung und das Framework um Modelle trainieren zu können und welche für diese Arbeit verwendet worden sind, sind sind im nachfolgend genanntem Github-Repository öffentlich einsehbar: \url{https://github.com/bjoern-hempel/keras-machine-learning-framework}\footnote{Keras Machine Learning Framework, Github, Febrary 21, 2020, \url{https://github.com/bjoern-hempel/keras-machine-learning-framework}}. Dieses Framework wurde in Python geschrieben, das es aus meiner Sicht die beste Programmiersprache für Maschinelles Lernen ist und schon viele vorgefertigte und hilfreiche Funktionen und Basis-Frameworks beinhaltet. Auch die Community mit ihren vielen Foren und Blogartikeln sind sehr breit aufgestellt. Als Python Maschine Learning Framework wurde Keras verwendet, da Keras als viele einheitliche Schnittstellen zu Backends wie TensorFlow enthält und das produzierte Modell auch in anderen Programmiersprachen wie Java weiterverwendet werden kann\footnote{Keras Machine Learning Framework (Java Sources), Github, Febrary 21, 2020, \url{https://github.com/bjoern-hempel/keras-machine-learning-framework-java-sources}}.}
			\en{All source code for the environment and the framework to train models and which has been used for this thesis is available in the following Github repository: \url{https://github.com/bjoern-hempel/keras-machine-learning-framework}\footnote{Keras Machine Learning Framework, Github, Febrary 21, 2020, \url{https://github.com/bjoern-hempel/keras-machine-learning-framework}}. This framework was written in Python, which in my opinion is the best programming language for machine learning and already contains many ready-made and helpful functions and basic frameworks. Also, the community with its many forums and blog articles is very broad. As Python Machine Learning Framework Keras was used, because Keras contains as many uniform interfaces to backends like TensorFlow and the produced model can also be reused in other programming languages like Java\footnote{Keras Machine Learning Framework (Java Sources), Github, Febrary 21, 2020, \url{https://github.com/bjoern-hempel/keras-machine-learning-framework-java-sources}}.}
			
			\de{Alle Modelle in den nachfolgenden Kapiteln wurden mit dem selbst geschriebenen Framework erstellt, welches erlaubt neben der Angabe des Trainingsdatenpfades auch alle hier genannten Parameter einzustellen: \url{http://bit.ly/2SNVnvE}\footnote{Keras Machine Learning Framework - Arguments, Github, Febrary 21, 2020, \url{https://github.com/bjoern-hempel/keras-machine-learning-framework/blob/master/markdown/image-classification/arguments.md\#user-content-arguments-of-the-training-process}}. Ein Kommandozeilenaufruf könnte nach erfolgreicher Installation mit den Standard-Werten z.B. wie folgt aussehen:}
			\en{All models in the following chapters were created with the self-written framework, which allows to set all parameters mentioned here in addition to the training data path: \url{http://bit.ly/2SNVnvE}\footnote{Keras Machine Learning Framework - Arguments, Github, Febrary 21, 2020, \url{https://github.com/bjoern-hempel/keras-machine-learning-framework/blob/master/markdown/image-classification/arguments.md\#user-content-arguments-of-the-training-process}}. A command line call with the default values could look like the following after successful installation:}

			\begin{verbatim}
					user$ ml train \
					  --use-train-val \
					  --data-path=./data/raw/food-50 \
					  --model-file=./data/processed/experiment1/type-of-experiment/model.h5
			\end{verbatim}
	
		\subsection{Performance}
			\de{Wenn man beabsichtigt ein Deep Neuronal Networks (DNN) zu implementieren und zu optimieren, müssen die Berechnungen auf der GPU erfolgen. Es ist auch möglich, Berechnungen auf der CPU durchzuführen. Auch die Installation von Keras für CPU-gesteuerte Berechnungen ist viel einfacher, da die Installation der GPU-Treiber nicht notwendig ist. Dies hat jedoch den Nachteil, dass das Training größerer Modelle wesentlich länger dauert. Gute Modelle für die Klassifizierung von z.B. Bildern werden erst nach mehreren Trainingseinheiten erreicht. Trainingseinheiten erfordern viel Rechenleistung in Form von vielen Matrixoperationen. Eine GPU ist prädestiniert für Matrixoperationen\autocite{fatahalian2004understanding}.}
			\en{If one intends to implement and optimize deep neuronal networks (DNN), the calculations must take place on the GPU. It is also possible to run calculations on the CPU. Also, the installation of Keras for CPU driven computations is much easier, because the installation of the GPU drivers is not necessary. The disadvantage of this, however, is that it takes much longer to train larger models. Good models for the classification of e.g. pictures are only achieved after several training units. Training units require a lot of computing power in the form of many matrix operations. A GPU is predestined for matrix operations\autocite{fatahalian2004understanding}.}
			
			\de{Nachfolgend folgt ein tabellarischer Leistungsvergleich auf der Grundlage des Kaggle-Blumendatensatzes Training\footnote{Flowers Recognition, Kaggle, Febrary 21, 2020, \url{https://www.kaggle.com/alxmamaev/flowers-recognition}}. Dieser Bildersatz besteht aus 5 Klassen mit etwa 4242 Trainingsbildern (10 epochs, InceptionV3)\footnote{GPU vs CPU, Github, Febrary 21, 2020, \url{https://github.com/bjoern-hempel/keras-machine-learning-framework/blob/master/markdown/hardware/gpu-vs-cpu.md}}:}
			\en{The following is a tabular comparison of performance based on the Kaggle flower data set training\footnote{Flowers Recognition, Kaggle, Febrary 21, 2020, \url{https://www.kaggle.com/alxmamaev/flowers-recognition}}. This image set consists of 5 classes with about 4242 training images (10 epochs, InceptionV3)\footnote{GPU vs CPU, Github, Febrary 21, 2020, \url{https://github.com/bjoern-hempel/keras-machine-learning-framework/blob/master/markdown/hardware/gpu-vs-cpu.md}}:}
			
			%\clearpage
			\renewcommand\theadfont{\bfseries}
			\begin{table}[htb]
				\small
				\centering
				{\def\arraystretch{2}\tabcolsep=5pt
					\begin{tabularx}{\linewidth}{ X | l | p{2cm} | l | l }
						\hline
						\thead[l]{Device} & \thead[l]{Preparation} & \thead[l]{Train} & \thead[l]{Train (Factor)} & \thead[l]{Save model} \\
						\hline
						NVIDIA GeForce GTX 1060 6GB (Desktop) - Windows & 17.5s & 303.5s\newline 00:05:03.5 & 1.00x & 33.8s \\
						NVIDIA GeForce GT 750M 2GB (Notebook) - Windows & 18.4s & 2415.0s\newline 00:40:15.0 & 7.96x & 29.8s \\
						Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz (Single Core) - MacOS & 19.1s & 6393.7s\newline 01:46:33.7 & 21.07x & 41.4s \\
						Intel(R) Core(TM) i7-4712HQ CPU @ 2.30GHz (Single Core) - Windows & 16.9s & 9016.8s\newline 02:30:16.8 & 29.71x & 28.7s \\
						Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz (Single Core) - Windows & 16.3s & 25183.4s\newline 06:59:43.4 & 82.97x & 28.3s
					\end{tabularx}
				}
				\captionof{table}{Performance comparison.}\label{tbl:table_performance_comparison}
			\end{table}
			
			\de{Während die Art des Rechengerätes (CPU or GPU) keinen Unterschied ausmacht was die Vor- und Nachbereitung betrifft, so macht es aber einen entscheidenden Unterschied bei der Trainingszeit. Die langsamste CPU benötigt mehr als 80 mal soviel Zeit, wie die schnellste Grafikkarteneinheit. Die Wahl des Rechengerätes für alle weiteren Tests fällt eindeutig auf die GPU.}
			\en{While the type of computing device (CPU or GPU) makes no difference in terms of preparation and postprocessing, it does make a significant difference in training time. The slowest CPU takes more than 80 times as many time as the fastest graphics card unit. The choice of the computing device for all further tests clearly falls on the GPU.}
	
		\subsection{Accuracy and evaluations}

			\subsubsection{Influence of number of trained images on accuracy}
			\label{sec:section_validation_number_of_train_files}
	
				\de{Die erste Frage die sich stellt: Welchen Einfluß hat die Anzahl der zu trainierenden Daten auf die Genauigkeit des Modells? Hierzu wird das Standard-Setup aus dem Kapitel ``\nameref{sec:section_validation_process_preamble}'' verwendet und das Modell mit einer verschiedenen Anzahl von Trainingsdaten trainiert. Während das Validierungsdatenset immer gleich bleibt, wird die Gesamtanzahl der zu trainierenden Daten von 500 stückweise auf die Gesamtanzahl von 11913 erhöht. Da es sich um einen unausgeglichenen Datensatz handelt, bleibt prozentuall gesehen die Anzahl in den jeweiligen Klassen gleich. Lediglich die Gesamtanzahl wird auf die entsprechenden Werte verändert (500, 1000, 2000, ...):}
				\en{The first question that comes up: What influence does the amount of data to be trained have on the accuracy of the model? For this purpose, the standard setup from the chapter ``\nameref{sec:section_validation_process_preamble}'' is used and the model is trained with a different number of training data. While the validation data set always stays the same, the total number of data to be trained is increased from 500 data sets to the total number of 11913. Since this is an unbalanced data set, the number remains the same in each class in percentage terms. Only the total number is changed to the corresponding values (500, 1000, 2000, ...):}
				
				\begin{figure}[H]
					\begin{center}
						\scalebox{0.75}{
							\inputpgf{images/evaluation}{number_train_files.pgf}
						}
					\end{center}
					\caption{Overview of influence of number of trained images on accuracy}
					\label{fig:evaluation_number_train_files}
				\end{figure}

				\de{Wie erwartet hat die Anzahl der zu trainierenden Bilder einen entscheidenden Einfluß auf die Modellgenauigkeit. Während mit 500 Bildern eine Genauigkeit von 45,76\% erreicht werden kann, sind es mit fast 12000 Bildern schon 83,59\% nach 21 Epochen. Mit jeder Erhöhung der Bildanzahl kann man eine Verbesserung der Genauigkeit feststellen. Der Anstieg der Genauigkeit erfolgt nicht linear mit Zunahme der Trainingsdatensätze. Obwohl die Genauigkeit im oberen Bereich (\textgreater8000 Bilder) immer noch zu steigen scheint, so sind die Sprünge nicht mehr sehr groß und bewegen sich offenbar an eine Grenze. Für weitere Nachforschungen wäre es eine gute Idee, ab welcher Anzahl von Trainingsdatensätzen keine weiteren signifikanten Steigerungen in der Genauigkeiten mehr möglich sind. Da in diesem Datenset keine weiteren Daten mehr zur Verfügung standen, wird dieses Vorhaben hier nicht weiter verfolgt.}
				\en{As expected, the number of images to be trained has a significant influence on the model accuracy. While with 500 images an accuracy of 45.76\% can be achieved, with almost 12000 images it is already 83.59\% after 21 epochs. With every increase in the number of images, an improvement in accuracy can be observed. The improvement in accuracy is not linear with an increase in training data sets. Although the accuracy in the upper range (\textgreater8000 images) still seems to increase, the jumps are not very big anymore and seem to be moving to a limit. For further research, it would be a good idea to find out from which number of training data sets no further significant increases in accuracy are possible. Since no further data was available in this data set, this project will not be pursued further here.}
				
				\de{Ein weiterer interessanter Punkt ist die Tatsache, dass mit der Erhöhung der Trainingsdaten theoretisch bis zu einem gewissen Punkt Rechenzeit eingespart werden kann. Sind die Datensätze nämlich deutlich zu klein, so erreicht man während des Trainings niemals eine Genaugkeit, welche Modelle mit wesentlich mehr Daten schon mit der ersten Epoche erreichen und übersteigen. Deutlich zu sehen: Bei 1000 Bildern (orangene Linie) kommt man maximal auf eine Genauigkeit von 61,42\% nach 18 Epochen und ungefähr 20 Minuten Rechenzeit. Verwendet man die 10-fache Menge an Bildern (10000 Bilder - braune Linie), so erreicht man schon in der ersten Epoche nach sechseinhalb Minuten eine Genauigkeit von 68,1\%.}
				\en{Another interesting point is the fact that increasing the training data theoretically saves computing time up to a certain point. If the data sets are significantly too small, one will never reach an accuracy during training, which models with much more data reach and exceed already with the first epoch. Clearly to see: With 1000 images (orange trace) one gets a maximum accuracy of 61.42\% after 18 epochs and about 20 minutes of computing time. If one uses 10 times the amount of images (10000 images - brown trace), one reaches an accuracy of 68.1\% already in the first epoch after six and a half minutes.}

			\subsubsection{Comparison of different CNN models}
			\label{sec:section_validation_comparison_cnn_models}
				\de{Derzeit gibt es viele CNN Modelle und jedes Jahr mit dem alljährlichem ImageNet-Wettbewerb kommen neue Modelle mit besseren Genauigkeiten heraus. Diese Modelle werden trainiert mit dem ImageNet Dataset und die Ergebnisse anhand dieser Datenbasis verglichen. Wie sieht das Ergebnis mit kleineren Datensets aus? Das Trainingsergebnis nach dem Standard-Setup und mit jeweils unterschiedlichem CNN Modell von acht Modellen sieht wie folgt aus:}
				\en{There are currently many CNN models and each year with the annual ImageNet competition new models with better accuracy are released. These models are trained with the ImageNet dataset and the results are compared using this set of data. What is the result with smaller sets of data? The training result according to the standard setup and with different CNN models of eight models is as follows:}

				\begin{figure}[H]
					\begin{center}
						\scalebox{0.75}{
							\inputpgf{images/evaluation}{different_models.pgf}
						}
					\end{center}
					\caption{Overview of known cnn models}
					\label{fig:evaluation_different_models}
				\end{figure}

				\de{Alle Modelle mit Ausnahme von DenseNet201 und NASNetLarge wurden mit einer Batch Size von 16 trainiert (\(bs=16\)). Aufgrund des begrenzten Speichers von 6GB der Nvidia GTX 1060\footnote{GeForce 10 series, Wikipedia contributors, February 22, 2020, \url{https://en.wikipedia.org/wiki/GeForce_10_series}} wurden DenseNet201 mit einer Batch Size von 8 und NASNetLarge mit einer Batch Size von 4 trainiert. Ein wirklicher Vergleich dieser beiden Ausnahmen ist somit nicht wirklich möglich, soll jedoch an dieser Stelle auch nicht weggelassen werden. Ein größerer Batch-Wert geht mit erhöhter Rechenzeit und langsameren Ansteigen von der Genauigkeit einher (siehe Kapitel ``\hyperref[sec:section_validation_comparison_different_bs]{Different batch sizes}''). Ein Vergleich mit Darstellung \fullref{fig:comparison_cnn} zeigt ähnliche Genauigkeiten und benötigte Rechenzeiten: Das beste Modell wurde mit DenseNet201 mit einer Genauigkeit von 88,58\% erreicht. Das Modell mit der geringsten Genauigkeit ist das schon etwas ältere VGG19 Modell mit 77,29\%. Stellt man Rechenzeit und zu erreichende Genauigkeiten gegenüber, so fällt einem das InceptionV3 Modell auf (rote Kurve). Es ist das zweitbeste Modell und erreicht das Ergebnis in einem mittlerem Bereich von ca. zweieinhalb Stunden. Das beste Modell DenseNet201 benötigt für sein Ergebnis mehr als doppelt soviel Zeit. Die Wahl für alle weiteren Experimente viel aus diesem Grund auf das CNN Modell InceptionV3, da es ein gutes Ergebnis innerhalb einer vergleichsweisen geringen Zeitspanne erreicht.}
				\en{All models except DenseNet201 and NASNetLarge were trained with a batch size of 16 (\(bs=16\)). Due to the limited 6GB memory of the Nvidia GTX 1060\footnote{GeForce 10 series, Wikipedia contributors, February 22, 2020, \url{https://en.wikipedia.org/wiki/GeForce_10_series}}, DenseNet201 was trained with a batch size of 8 and NASNetLarge with a batch size of 4. A real comparison of these two exceptions isn't really possible, but shouldn't be omitted at this point. A larger batch value is associated with increased computing time and slower increase in accuracy (see chapter ``\hyperref[sec:section_validation_comparison_different_bs]{Different batch sizes}''). A comparison with \fullref{fig:comparison_cnn} shows similar accuracies and required computing time: The best model was achieved with DenseNet201 with an accuracy of 88.58\%. The model with the lowest accuracy is the somewhat older VGG19 model with 77.29\%. If one compares the computing time and the accuracies to be achieved, one notices the InceptionV3 model (red trace). It is the second best model and reaches the result in a medium range of about two and a half hours. The best model DenseNet201 needs more than twice as much time for its result. The choice for all further experiments is therefore the CNN model InceptionV3, because it achieves a good result within a comparatively short period of time.}
		
			\subsubsection{Use of the transfer learning approach}
			\label{sec:section_use_of_the_transfer_learning_approach}
	
				\de{Welchen Einfluss auf die Genauigkeit hat das Verwenden von Transfer Learning? Wie im Kapitel ``\hyperref[sec:section_transfer_learning]{Transfer learning}'' beschrieben, verbessert der Transfer Learning Ansatz sofort die Genauigkeit, da das Modell schon Erkennungsmerkmale besitzt, welche ein ungelerntes Netzwerk erst noch lernen muss. Das Ergebnis sieht wie folgt auf den Datensatz \texttt{food-50} aus:}
				\en{What influence does the use of transfer learning have on accuracy? As described in the chapter ``\hyperref[sec:section_transfer_learning]{Transfer learning}'', the transfer learning approach immediately improves accuracy because the model already has recognition features that an unlearned network has yet to learn. The result looks like this on the data set \texttt{food-50}:}
				
				\begin{figure}[H]
					\begin{center}
						\scalebox{0.75}{
							\inputpgf{images/evaluation}{transfer_learning.pgf}
						}
					\end{center}
					\caption{Overview of use of the transfer learning approach}
					\label{fig:evaluation_transfer_learning}
				\end{figure}

				\de{Wie erwartet erziehlt das Modell mit Transfer Learning ein besseres Ergebnis bei gleicher Rechenzeit. In der ersten Epoche erreicht das Modell ohne Transfer Learning Ansatz einen Genauigkeitswert von 8,5\% und steigert sich in den 21 Lernepochen auf 58,8\%. Beginnt man mit einem trainierten Netzwerk, so erreicht man schon mit der ersten Epoche einen besseren Wert von 68,9\% und kann diesen weiter steigern bis 83,6\% in der 21. Epoche. Als Fazit sei zu erwähnen, dass es sicher möglich ist auch das untrainierte Netzwerk auf diesen Wert zu bekommen, jedoch benötigt man deutlich mehr Trainingsepochen. Transfer Learning zu verwenden ist in diesem Fall eine gute Idee.}
				\en{As expected, the transfer learning model achieves a better result with the same computing time. In the first epoch the model without transfer learning approach reaches an accuracy value of 8.5\% and increases to 58.8\% in the 21 learning epochs. If one starts with a trained network, one achieves a better value of 68.9\% already in the first epoch and can further increase this value to 83.6\% in the 21st epoch. As a conclusion it should be mentioned that it is certainly possible to get the untrained network to this value, but one needs much more training epochs. Using transfer learning is a good idea in this case.}
				
			\subsubsection{Influence of the number of trained layers on the accuracy}
				\de{Ein vortrainiertes Netzwerk hat einen entscheidenden Einfluss auf die zu erreichende Genauigkeit, die man in einer gewissen Rechenzeit erreichen kann. Mit fortschreitender Tiefe der Ebenen eines CNN Modells, haben die dazugehörigen Filter mehr und mehr komplexere Erkennungsmerkmale gelernt\footnote{Advanced Topics in Deep Convolutional Neural Networks, https://towardsdatascience.com, February 22, 2020, \url{https://towardsdatascience.com/advanced-topics-in-deep-convolutional-neural-networks-71ef1190522d}}: In den ersten Ebenen lernen die Filter grundlegende Formen zur Erkennung von Merkmalen wie Kanten und Ecken. Die mittleren Ebenen lernen Teile von Objekten zu erkennen. Die letzten Ebenen lernen vollständige Objekte in verschiedenen Formen und Positionen. Die Frage welche sich unmittelbar stellt: Ist es notwendig die unteren Schichten neu zu erlernen oder ist es möglich diese auszulassen, um Rechenzeit einzusparen? Dies soll im folgenden Experiment überprüft werden:}
				\en{A pre-trained network has a decisive influence on the accuracy that can be achieved in a certain computing time. With increasing depth of the layers of a CNN model, the associated filters have learned more and more complex recognition features\footnote{Advanced Topics in Deep Convolutional Neural Networks, https://towardsdatascience.com, February 22, 2020, \url{https://towardsdatascience.com/advanced-topics-in-deep-convolutional-neural-networks-71ef1190522d}}: In the first layers, the filters learn basic shapes to recognize features such as edges and corners. The middle layers learn to recognize parts of objects. The last layers learn complete objects in different shapes and positions. The question that immediately comes up: Is it necessary to relearn the lower layers or is it possible to omit them to save computing time? This will be tested in the following experiment:}

				\begin{figure}[H]
					\begin{center}
						\scalebox{0.75}{
							\inputpgf{images/evaluation}{number_trainable_layers.pgf}
						}
					\end{center}
					\caption{Overview of influence of the number of trained layers}
					\label{fig:evaluation_number_trainable_layers}
				\end{figure}

				\de{Die Idee auf das Training der untersten Ebenen zu verzichten ist leider keine gute Idee. Zwar spart man enorm Rechenzeit, wenn man nur die letzten 36 Ebenen trainiert (ungefähr eine Stunde Rechenzeit gegenüber zweieinhalb Stunden, wenn man alle Ebenen trainiert), jedoch steigt auch die Modellgenauigkeit mit dem Training von weiteren zusätzlichen Ebenen stetig. Die Modellgenauigkeit konnte beim Training von allen Ebenen von 64,8\% auf 83,6\% verbessert werden. Es ist eine gute Idee in Rechenzeit zu investieren, um gute Modelle zu erhalten.}
				\en{The idea of not training the lowest layers is unfortunately not a good idea. One can save a lot of computing time if one only trains the last 36 levels (about one hour of computing time compared to two and a half hours if one trains all levels), but the accuracy of the model increases steadily with the training of additional layers. The model accuracy could be improved from 64.8\% to 83.6\% when training all levels. It is a good idea to invest in computing time to get good models.}

			\subsubsection{Influence of different error optimizers}
				\de{Der Optimierer im Trainingsprozess dient dazu den Wert der Verlustfunktion zu verringern, um den Wert der Vorhersagen so korrekt wie möglich wiederzugeben. Der Wert der Verlustfunktion ergibt sich aus der Differenz des erwarteten Wertes und des geschätzten Wertes der Funktion. Optimierer bestimmen den Wert und begleichen den Fehler um die Verlustfunktion zu minimieren. Der einfachste Ansatz ist das Gradientenabstiegsverfahren\footnote{Gradient descent, Wikipedia contributors, February 22, 2020, \url{https://en.wikipedia.org/wiki/Gradient_descent}}, welches im Laufe der Zeit immer mehr verbessert wurde und weitere Optimierungsverfahren entstanden, welche entsprechend ihrer Aufgaben und Ziele mal besser und mal nicht so gut funktionieren. Die Lernrate \(\eta\) bestimmt den Wert der Gewichtung, mit dem der Fehler korrigiert wird (siehe auch Kapitel ``\hyperref[sec:section_loss_function]{Loss function}''):}
				\en{The optimizer in the training process is used to reduce the value of the loss function to reflect the value of the predictions as correctly as possible. The value of the loss function is the difference between the expected value and the estimated value of the function. Optimizers determine the value and adjust the error to minimize the loss function. The simplest approach is the gradient descent algorithm\footnote{Gradient descent, Wikipedia contributors, February 22, 2020, \url{https://en.wikipedia.org/wiki/Gradient_descent}}, which has been improved over time and further optimization algorithms have been developed, which sometimes work better and sometimes not so well according to their tasks and goals. The learning rate \(\eta\) determines the value of the weighting with which the error is corrected (see also chapter ``\hyperref[sec:section_loss_function]{Loss function}''):}

				\begin{equation}
					x_{n+1} = x_{n} - \eta \cdot \nabla F(x_{n}) = x_{n} - \eta \cdot L_r(\vartheta, \lambda), \ n \geq 0
				\end{equation}
				
				\de{\noindent Dieser Bereich beschäftigt sich mit dem Thema Optimierer und seinen Parametern (besonders der Lernrate \(\eta\)) und welchen entscheidenden Einfluss diese auf die Modellgenauigkeit haben.}
				\en{\noindent This area deals with the topic of optimizers and their parameters (especially the learning rate \(\eta\)) and what decisive influence they have on the accuracy of the model.}

				\paragraph{Comparison optimizer}
				\label{sec:section_validation_comparison_optimizer}
					\de{Neben dem Gradientenabstiegsverfahren gibt es eine Menge weiterer Verfahren, welche den Abstieg zum Optimum verbessern sollen. Dabei werden zum Beispiel Techniken verwendet, die zusammen mit der Lernrate das Springen zwischen dem Optimum verhindern oder einschränken und somit das Konvergieren optimieren\footnote{A Look at Gradient Descent and RMSprop Optimizers, https://towardsdatascience.com, February 22, 2020, \url{https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b}}. Nachfolgend der Vergleich aktueller Optimierungsverfahren:}
					\en{In addition to the gradient descent algorithm, there are a lot of other methods which are intended to improve the descent to the optimum. For example, techniques are used which, together with the learning rate, prevent or limit jumping between the optimum and thus optimize convergence\footnote{A Look at Gradient Descent and RMSprop Optimizers, https://towardsdatascience.com, February 22, 2020, \url{https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b}}. Below is a comparison of current optimization methods:}
					
					\begin{figure}[H]
						\begin{center}
							\scalebox{0.75}{
								\inputpgf{images/evaluation}{best_optimizer.pgf}
							}
						\end{center}
						\caption{Overview of best optimizer}
						\label{fig:evaluation_momentum}
					\end{figure}
	
					\de{Wie man sehen kann, fällt das Erbebnis sehr unterschiedlich aus. Während das Gradientenabstiegsverfahren mit und ohne Nesterov und das Adagrad Verfahren gute Werte liefern (82,0\% bis 83,6\% Genauigkeit) sehen die anderen Verfahren auf den ersten Blick nicht sehr brauchbar aus. Sie konvergieren sehr langsam: Adadelta, Adam und RMSprop. Alle Verfahren wurden entsprechend der Keras Dokumentation mit den empfohlenen Einstellungen durchgeführt\footnote{Usage of optimizers, https://keras.io, February 22, 2020, \url{https://keras.io/optimizers/}}. Bei allen Verfahren wurde eine Lernrate von \(\eta = 0.001\) gewählt.}
					\en{As one can see, the results are very different. While the gradient descent method with and without Nesterov and the Adagrad method provide good values (82.0\% to 83.6\% accuracy), the other methods do not look very useful at first sight. They converge very slowly: Adadelta, Adam and RMSprop. All procedures were performed with the recommended settings according to the Keras documentation\footnote{Usage of optimizers, https://keras.io, February 22, 2020, \url{https://keras.io/optimizers/}}. A learning rate of \(\eta = 0.001\) was chosen for all procedures.}

				\paragraph{Influence of the momentum and the Nesterov momentum}
					\de{Die beiden besten Verfahren aus dem vorhergehendem Kapitel ``\hyperref[sec:section_validation_comparison_optimizer]{Comparison optimizer}'' SGD mit Nesterov und SGD ohne Nesterov sollen hier genauer untersucht werden. Der Lernparameter \(\eta\) spielt dabei eine entscheidende Rolle. Dieser wird von 0,5 bis in den kritischen Bereich von 0,98 variiert. Die Lernrate verringert sich nach jeweils 7 Epochen um 50\%. Das Ergebnis soll daraufhin verglichen und diskutiert werden.}
					\en{The two best methods from the previous chapter ``\hyperref[sec:section_validation_comparison_optimizer]{Comparison optimizer}'' SGD with Nesterov and SGD without Nesterov will be examined here in more detail. The learning parameter \(\eta\) plays a decisive role. It is varied from 0.5 to the critical range of 0.98. The learning rate decreases by 50\% after every 7 epochs. The results will then be compared and discussed:}
	
					\begin{figure}[H]
						\begin{center}
							\scalebox{0.75}{
								\inputpgf{images/evaluation}{momentum.pgf}
							}
						\end{center}
						\caption{Overview momentum vs Nesterov momentum}
						\label{fig:evaluation_momentum}
					\end{figure}
	
					\de{Die Genauigkeit steigen bei beiden Modellen mit (w/ nest.) und ohne Nesterov (w/o nest.) stetig mit steigender Lernrate \(\eta\) an, bis sie mit \(\eta=0,9\) (w/o nest.) bzw. \(\eta=0,95\) (w/ nest.) ihren Zenit bei 83,35\% bzw. 84,37\% erreichen. Dabei fällt auf, das je kleiner die Lernrate \(\eta\) ist, die Genauigkeit vorhersehbarer gleichmäßiger steigt, das ``Maximum'' bei 21 Epochen jedoch nicht erreicht wird. Mit steigender Lernrate, z.B. beim Maximum von \(\eta=0,98\) beim Nesterov Modell (braune Linie), springen die Modellgenauigkeiten und können auch mal mit einer weiteren Epoche komplett in die negative Richtung laufen. Beim besten Modell (olive Linie, w/ nest.) passiert exakt das Gleiche, jedoch werden am Ende damit die besten Ergebnisse erzielt.}
					\en{The accuracy of both models with (w/ nest.) and without Nesterov (w/o nest.) increases steadily with increasing learning rate \(\eta\) until they reach their zenith at 83.35\% and 84.37\% with \(\eta=0,9\) (w/o nest.) and \(\eta=0,95\) (w/ nest.) respectively. It is noticeable that the smaller the learning rate \(\eta\), the accuracy increases more evenly, but the ``maximum'' at 21 epochs is not reached. With increasing learning rate, e.g. at the maximum of \(\eta=0,98\) for the Nesterov model (brown trace), the model accuracies jump and can sometimes run completely in the negative direction with another epoch. With the best model (olive trace, w/ nest.) exactly the same happens, but in the end the best results are achieved.}
		
				\paragraph{Influence of a dynamic learning rate on accuracy (scheduling)}
					\de{Die Lernrate \(\eta\) gibt an, wieviel vom Fehler in das Modell zurückgegeben wird (Schrittweite). Ab einer bestimmten Anzahl von Lernepochen steigt die Modellgenauigkeit nicht mehr an, sondern springt um einen Wert herum (siehe grüne Linie in der nachfolgendem Abbildung), weil man aufgrund einer zu großen Schrittweite bei der Fehlerkorrektur das Optimum nicht erreichen kann. Es ist also eine gute Idee die Schrittweite im Laufe der Epochen schrittweise anzupassen und zu verkleinern. Hierzu wird die Lernrate \(\eta\) nach einer gewissen Anzahl von Epochen \(\mathcal{E}\) (z.B. alle 7 Epochen) mit dem Momentum \(\beta\) angepasst. Der Wert für \(\eta_1\) entspricht dem initialem Wert für die Lernrate:}
					\en{The learning rate \(\eta\) indicates how much of the error is returned to the model (step size). After a certain number of learning epochs, the model accuracy does not increase any more but jumps around a value (see green trace in the following figure), because the optimum cannot be achieved due to a too large step size in error correction. It is therefore a good idea to adjust and reduce the step size step by step over the epochs. For this purpose the learning rate \(\eta\) is adjusted after a certain number of epochs \(\mathcal{E}\) with the momentum \(\beta\). The value for \(\eta_1\) corresponds to the initial value for the learning rate:}
					
					\begin{equation}
						\eta_{m\to next} = \beta \cdot \eta_{m}, \ m \in [1 + 0 \cdot \mathcal{E}, 1 + 1 \cdot \mathcal{E}, 1 + 2 \cdot \mathcal{E}, \dots]
					\end{equation}
	
					\begin{figure}[H]
						\begin{center}
							\scalebox{0.75}{
								\inputpgf{images/evaluation}{scheduling_learning_rate.pgf}
							}
						\end{center}
						\caption{Overview of a dynamic learning rate on accuracy}
						\label{fig:evaluation_scheduling_learning_rate}
					\end{figure}
	
					\de{Wie vermutet verbessert eine Verringerung der Lernrate über die Zeit die mögliche Modellgenauigkeit. Jedoch darf sie auch nicht zu stark verringert werden, da dies die weitere Lernmöglichkeit zu sehr einzuschränkt. Während ein Momentum von \(\beta=0.5\) noch eine Modellgenauigkeit von 83,6\% liefert, verringert sich diese bei \(\beta=0.1\) auf 83,0\%. Eine statische Lernrate verbessert die Lernfähigkeit ab ungefähr der 8. Epoche nicht mehr. Sie springt dann um den Wert von 81\% und scheint sich sogar ein wenig zu verschlechtern. Das Momentum gehört zur Gruppe der Hyperparameter\footnote{Hyperparameter optimization, Wikipedia contributors, February 22, 2020, \url{https://en.wikipedia.org/wiki/Hyperparameter_optimization}} und muss experimentell ermittelt werden. Ein Erfahrungswert kann als Startwert dienen.}
					\en{As suspected, a reduction of the learning rate over time improves the possible model accuracy. However, it must not be reduced too much, as this would limit the further learning possibilities too much. While a momentum of \(\beta=0.5\) still provides a model accuracy of 83.6\%, this decreases to 83.0\% at \(\beta=0.1\). A static learning rate does not improve the learning ability from about the 8th epoch onwards. It then jumps around the value of 81\% and even seems to worsen a little. Momentum belongs to the group of hyperparameters\footnote{Hyperparameter optimization, Wikipedia contributors, February 22, 2020, \url{https://en.wikipedia.org/wiki/Hyperparameter_optimization}} and must be determined experimentally. An empirical value can be used as a starting value.}
	
			\subsubsection{Different batch sizes}
			\label{sec:section_validation_comparison_different_bs}
	
				\noindent ...
		
			\subsubsection{Different activation functions}
	
				\noindent ...
		
			\subsubsection{Different number of learned epochs}
	
				\noindent ...
		
			\subsubsection{Influence of dropout}
	
				\noindent ...


	
	% -------------------- %
	% Optimization process %
	% -------------------- %
	\section{Optimization process}
		\de{Neben den Hyperparametern (oder auch Tuning-Parametern) gibt es noch eine Reihe anderer Möglichkeiten, welche Einfluss auf die Modellgenauigkeit haben. Zu erwähnen sei z.B. das Problem in Kapitel ``\hyperref[sec:section_validation_number_of_train_files]{Influence of the number of trained layers on the accuracy}'', bei der aufgrund fehlender Daten die Modellgenauigkeit mit mehr als den vorhandenen 12000 Bildern nicht weiter untersucht werden konnte, obwohl die Genauigkeit noch weiter anzusteigen schien. Oder das Problem mit dem unbalanciertem Datenset. Wäre es nicht eine Idee in allen Klassen gleich viel Daten zu haben, ohne Daten zu verwerfen oder zusätzlich besorgen zu müssen? Was ist mit anderen Klassifizierungsideen? Momentan wird ein Modell für alle Klassen verwendet. Machen Hierarchien Sinn? All diese Dinge sollen in den nachfolgenden Kapiteln genauer untersucht werden.}
		\en{In addition to hyperparameters (or tuning parameters), there are a number of other options that influence model accuracy. It should be mentioned, for example, the problem in chapter ``\hyperref[sec:section_validation_number_of_train_files]{Influence of the number of trained layers on the accuracy}'', where the model accuracy could not be further investigated with more than the existing 12000 images due to missing data, although the accuracy seemed to increase even more. Or the problem with the unbalanced data set. Wouldn't it be an idea to have the same amount of data in all classes without having to discard data or get additional data?  What about other classification ideas? Currently, one model is used for all classes. Do hierarchies make sense? All these things will be examined in the following chapters.}
	
		\subsection{Preamble}
			\de{Wie auch in Kapitel ``\hyperref[sec:section_validation_process_preamble]{Preamble}'' bei der Modellvalidierung werden auch hier die gleichen Einstellungen als Standard-Setup verwendet, sofern nicht anders angegeben. Als CNN kommt InceptionV3 zum Einsatz. Das Datenset und auch die Aufteilung in Trainings- und Validierungsdatenset entspricht der Vorgehensweise von den genannten Vorkapiteln.}
			\en{As in Chapter ``\hyperref[sec:section_validation_process_preamble]{Preamble}'' for Model Validation, the same settings are used as the default setup, unless otherwise specified. InceptionV3 is used as CNN. The data set and also the division into training and validation data set corresponds to the procedure of the previous chapters.}

		\subsection{Data augmentation}
			\de{Wie in Kapitel ``\hyperref[sec:section_data_augmentation]{Data augmentation}'' beschrieben, ist die Hauptaufgabe von Data augmentation neue neue Trainingsdaten aus den bestehenden zu generieren. Der Trainingsdatensatz wird somit künstlich vergrößert. Diese Technik gehört mit zu den Regularisierungstechniken, da sie Overfitting entgegenwirken kann\autocite{geron2017supervisedlearningDataAugmentation}. Nachfolgend ein Beispieldatenset, bei dem das erste originale Bild gedreht, gespiegelt, verzerrt wird und der Datensatz von einem Bild auf insgesamt 12 Bilder vergrößert wird:}
			\en{As described in Chapter ``\hyperref[sec:section_data_augmentation]{Data augmentation}'' the main task of data augmentation is to generate new training data from the existing data. The training data set is artificially augmented in this way. This technique is one of the regularization techniques, as it can counteract overfitting\autocite{geron2017supervisedlearningDataAugmentation}. Below is an example data set in which the first original image is rotated, mirrored, distorted and the data set is enlarged from one image to a total number of 12 images:}

			\begin{figure}[H]
				\begin{center}
					\inputpgf{images/pgf}{augment.pgf}
				\end{center}
				\caption{Data Augmentation (The first image is the original image.)}
				\label{fig:data_augmentation}
			\end{figure}
			
			\de{Mit dieser Technik wird nun der bestehende \texttt{food-50} Datensatz angepasst, bis jede Klasse insgesamt 1000 Bilder enthält und der Datensatz insgesamt ausbalanciert ist. Klassen mit wenigen Daten erhalten entsprechend mehr künstlich angepasste Daten, Klassen mit vielen Daten entsprechend weniger. Das Ergebnis ist, dass aus den ursprünglich 11913 Bildern nun insgesamt 50000 Bilder wurden und der Datensatz von insgesamt 765 MB auf 1,78 GB angewachsen ist. Der Validierungsdatensatz bleibt unverändert, um das Testergebnis nicht zu verfälschen. Nachfolgend das Auswertungsdiagramm:}
			\en{Using this technique, the existing \texttt{food-50} dataset is now adjusted until each class contains a total number of 1000 images and the dataset is balanced overall. Classes with few data will receive more artificially adjusted data, classes with many data will receive less. The result is that the original 11913 images have now become a total number of 50000 images and the data set has grown from 765 MB to 1.78 GB. The validation data set remains unchanged in order not to falsify the test result. The evaluation diagram is shown below:}
			
			\begin{figure}[H]
				\begin{center}
					\scalebox{0.75}{
						\inputpgf{images/evaluation}{data_augmentation.pgf}
					}
				\end{center}
				\caption{Comparison of data augmentation}
				\label{fig:evaluation_data_augmentation}
			\end{figure}
			
			\de{Die Modellgenauigkeit erhöht sich von 83,6\% auf 85,6\%. Eine Steigerung von insgesamt 2\%. Wohlgemerkt: Der Validierungsdatensatz ist immer noch derselbe! Allerdings bedeutet das auch ein Anstieg der benötigten Rechenzeit. Während die Berechnung des Datensatzes ohne Data Augmentation nur zweieinhalb Stunden braucht, benötigt man für den neuen Datensatz die vierfache Zeit. Fazit: Data Augmentation geth mit einer verbesserten Genauigkeit einher, benötigt aber auch im gleichem Atemzug mehr Rechenzeit. Wenn Rechenzeit keine Rolle spielt, so ist dies eine Möglichkeit die Genauigkeit etwas zu verbessern.}
			\en{The model accuracy increases from 83.6\% to 85.6\%. An overall increase of 2\%. Note: The validation data set is still the same! However, this also means an increase in the required computing time. While the calculation of the data set without data augmentation takes only two and a half hours, the new data set takes four times as long. Conclusion: Data Augmentation is associated with improved accuracy, but also requires more computing time in the same breath. If computing time is not important, this is one way to improve the accuracy a little.}

		\subsection{Enrichment of the data set from other data sources}
			\noindent ...
		
		\subsection{Analyses with multidimensional scaling}
			\noindent ...
		
		\subsection{Hierarchical classification}
		\label{sec:section_validation_hierarchical classification}
			\de{Durch die Verwendung eines einzigen Modelles für alle Klassen, sind die bisherigen Klassifikatoren darauf trainiert, den Verlust am Klassenausgabevektor zu minimieren. Jede bisher verwendete Klasse hat den gleichen Rang sowohl beim Training, als auch bei der Klassifizierung. Die Vorhersage von "Pizza" kostet genauso viel wie die Vorhersage von "Martini".}
			\en{By using a single model for all classes, previous classifiers have been trained to minimize the loss of the class output vector. Each class used so far has the same rank in both training and classification. The prediction of "Pizza" costs the same as the prediction of "Martini".}

			\de{Die menschliche Fähigkeit Objekte einordnen zu können, funktioniert nicht nur auf einer Ebene. Kategorien werden sich natürlich überlappen und eine hierarchische Struktur aufweisen. So wird ein Mensch ein Bild beispielsweise unter "Pizza", "Thunfischpizza" oder sogar "Fastfood" einordnen, was so gesehen korrekt ist. Je nach Einordnung findet hier lediglich ein "Informationsverlust" statt. Jedoch wird der Mensch eine "Pizza" meist nicht fälschlicherweise als "Martini" verwechseln, welcher eher der Kategorie "Getränk" oder "Cocktail" einzuordnen ist\autocite{rosch2004basic}.}
			\en{The human ability to classify objects does not only work on one level. Categories will naturally overlap and have a hierarchical structure. For example, a human will classify a picture under "pizza", "tuna pizza" or even "fast food", which is correct from this point of view. Depending on the classification, there will only be a "loss of information". However, a person will not mistake a "pizza" as a "Martini", which is more likely to be classified as a "drink" or "cocktail"\autocite{rosch2004basic}.}
			
			\de{Um herauszufinden, ob eine Vorklassifizierung das Ergebnis verbessert, werden die Klassen in Gruppen aufgeteilt. Die Idee dabei ist, dass nicht mehr ein Modell alle Klassen vorhersagt, sondern die Bilder erstmal in eine Klasse vorsortiert werden. Die entsprechende Gruppe nimmt dann die eigentliche Klassifizierung vor. Dazu wird vorbereitend eine Principal component analysis vorgenommen, um die Ähnlichkeit der Klassen aus dem \texttt{food-50} Trainingsdatensatz zu analysieren. Mit dem Ergebnis werden dann Gruppen erstellt. Vorbereitend wird erneut ein Modell für alle Klassen trainiert: InceptionV3, 21 Epochen, Batch Size 16, Lernrate \(\eta = 0.001\) absteigend um Faktor 0.5 alle 7 Epochen. Mittels diesem Modell wird von allen Trainings- und Validierungsdaten eine Vorhersage durchgeführt und der Wahrscheinlichkeitsvektor bestimmt (One Hot Encoding):}
			\en{In order to find out whether a pre-classification improves the result, the classes are divided into groups. The idea is that no longer one model predicts all classes, but rather the images are pre-sorted into a group. The corresponding group then carries out the actual classification. For this purpose, a principal component analysis is performed to analyze the similarity of the classes from the \texttt{food-50} training data set. The result will then be used to create groups. In preparation, a model will again be trained for all classes: InceptionV3, 21 epochs, batch size 16, learning rate \(\eta = 0.001\) decreasing by factor 0.5 every 7 epochs. Using this model, a prediction is made of all training and validation data and the probability vector is determined (One Hot Encoding):}
			
				\begin{equation}
					\boldsymbol\lambda_{class_{m}} = 
					\left(
						\begin{array}{c}
						\hat{p}_{1}\\
						\hat{p}_{2}\\
						\vdots\\
						\hat{p}_{50}\\
						\end{array}
					\right)
					\quad\Biggl\lvert \quad \sum_{i=1}^{50} \hat{p_i} = 1
				\end{equation}
				
				\de{Von diesen Wahrscheinlichkeitsvektoren wird nun jeweils für jede Klasse ein Durchschnittsvektor aller \(n\) Elemente bestimmt, welche zur entsprechenden Klasse wirklich gehören. Man erhält also für dieses 50 Klassenmodell entsprechend 50 Klassenvektoren mit der Dimension 50:}
				\en{From these probability vectors, an average vector of all \(n\) elements is determined for each class, which really belong to the respective class. Thus, for this 50 class model, one obtains 50 class vectors with the dimension 50:}
				
				\begin{equation}
					\forall\ 50\ classes:
					\overline{\boldsymbol\lambda}_{class_{m}} =
					\frac{
						\sum_{i=1}^{n}
						\boldsymbol\lambda_{class_{m}}
					}{n}
				\end{equation}
				
				\de{Mittels Principal component analysis werden diese mehrdimensionalen Vektoren in einem zweidimensionalen Raum transformiert, um diese grafisch darstellen zu können:}
				\en{Principal component analysis is used to transform these multidimensional vectors into a two-dimensional space in order to display them visually:}
			
				\begin{figure}[H]
					\begin{center}
						\scalebox{0.5}{
							\inputpgf{images/pgf}{pca.pgf}
						}
					\end{center}
					\caption{Principal component analysis of the \texttt{food-50} model. The x and y axis have no significance. They only stand for the room and the similarity between the classes.}
					\label{fig:analysis_pca_food_50}
				\end{figure}
				
				\de{Wie erwartet kann man nun sofort ähnliche Klassen erkennen: Getränke (orange), Kuchen (blau) und Salate (grün).}
				\en{As expected, one can now immediately recognize similar classes: drinks (orange), cakes (blue) and salads (green).}
				
				\subsubsection{\(k\)-means clustering}
			
					\begin{figure}[H]
						\begin{center}
							\scalebox{0.5}{
								\inputpgf{images/pgf}{pca-grouped.pgf}
							}
						\end{center}
						\caption{Grouped classes with k-means.}
						\label{fig:analysis_grouped_classes_with_k_means}
					\end{figure}
					
					%\clearpage
					\renewcommand\theadfont{\bfseries}
					\begin{table}[htb]
						\small
						\centering
						{\def\arraystretch{2}\tabcolsep=5pt
							\begin{tabularx}{\linewidth}{ l | X }
								\hline
								\thead[l]{Group} & \thead[l]{Classes} \\
								\hline
								Group 0 & baked\_salmon, calzone, chicken\_piccata, corn\_dog, empanada, french\_fries, frittata, grilled\_cheese\_sandwich, kebabs, lasagne, meatloaf, omelet, pancakes, pizza, popcorn, stuffed\_pepper, waffles \\
								Group 1 & margarita, smoothie \\
								Group 2 & bundt\_cake, buttermilk\_biscuits, cheesecake, granola\_bar, muffin \\
								Group 3 & brownies \\
								Group 4 & martini \\
								Group 5 & cinnamon\_roll, donut, ice\_cream, key\_lime\_pie \\
								Group 6 & baked\_beans, beef\_stew, beef\_stroganoff, caesar\_salad, cobb\_salad, coleslaw, creamed\_spinach, guacamole, macaroni\_and\_cheese, mashed\_potatoes, nachos, salad, soup \\
								Group 7 & burger, burrito, chicken\_wings, meatballs, quesadilla, sloppy\_joe, spaghetti
							\end{tabularx}
						}
						\captionof{table}{Grouped classes with k-means.}\label{tbl:table_grouped_classes}
					\end{table}
				
				\subsubsection{Agglomerative hierarchical clustering}
				
					\begin{figure}[H]
						\begin{center}
							\scalebox{0.5}{
								\inputpgf{images/pgf}{pca-grouped-ahc.pgf}
							}
						\end{center}
						\caption{Grouped classes with agglomerative hierarchical clustering.}
						\label{fig:analysis_grouped_classes_with_ahc}
					\end{figure}
					
					%\clearpage
					\renewcommand\theadfont{\bfseries}
					\begin{table}[htb]
						\small
						\centering
						{\def\arraystretch{2}\tabcolsep=5pt
							\begin{tabularx}{\linewidth}{ l | X }
								\hline
								\thead[l]{Group} & \thead[l]{Classes} \\
								\hline
								Group 0 & baked\_beans, beef\_stew, beef\_stroganoff, burger, burrito, caesar\_salad, chicken\_piccata, chicken\_wings, cobb\_salad, coleslaw, creamed\_spinach, french\_fries, frittata, kebabs, macaroni\_and\_cheese, meatballs, nachos, quesadilla, salad, sloppy\_joe, soup, spaghetti \\
								Group 1 & bundt\_cake, buttermilk\_biscuits, cheesecake, granola\_bar, muffin \\
								Group 2 & guacamole, ice\_cream, mashed\_potatoes, popcorn \\
								Group 3 & martini \\
								Group 4 & margarita, smoothie \\
								Group 5 & baked\_salmon, calzone, corn\_dog, empanada, grilled\_cheese\_sandwich, lasagne, meatloaf, omelet, pancakes, pizza, stuffed\_pepper, waffles \\
								Group 6 & cinnamon\_roll, donut, key\_lime\_pie \\
								Group 7 & brownies
							\end{tabularx}
						}
						\captionof{table}{Grouped classes with agglomerative hierarchical clustering.}\label{tbl:table_grouped_classes_ahc}
					\end{table}
					
					\de{}
					\en{}
		
		\subsection{Binary classifiers}
			\noindent ...
		
		\subsection{Evaluation}
			\noindent ...
		
		\subsection{Use of the model across programming languages}
			\noindent ...



	% -------------------- %
	% Summary and outlook %
	% -------------------- %
	\pagebreak
	\section{Summary and outlook}
		\hl{What's the outcome? What else is possible? How can this work be continued? In here!}


	% -------------------- %
	% List of figures %
	% -------------------- %
	\pagebreak
	\renewcommand{\listfigurename}{List of figures}
	\addcontentsline{toc}{section}{List of figures}
	\listoffigures
	\listoftables
	


	% -------------------- %
	% List of literature %
	% -------------------- %
	\pagebreak	
	\section*{List of literature}
		\addcontentsline{toc}{section}{List of literature}
		\printbibliography[heading=none]
	


	% -------------------- %
	% List of literature %
	% -------------------- %
	\pagebreak	
	\section*{List of links}
		\addcontentsline{toc}{section}{List of links}
		\begin{itemize}
			\item Deep learning unbalanced training data?
			\begin{itemize}
				\item \url{https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6}
			\end{itemize}
			\item Data Augmentation
			\begin{itemize}
				\item \url{https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/}
			\end{itemize}
			\item Stop Feeding Garbage To Your Model! — The 6 biggest mistakes with datasets and how to avoid them.
			\begin{itemize}
				\item \url{https://hackernoon.com/stop-feeding-garbage-to-your-model-the-6-biggest-mistakes-with-datasets-and-how-to-avoid-them-3cb7532ad3b7}
			\end{itemize}
			\begin{itemize}
				\item \url{https://towardsdatascience.com/advanced-topics-in-deep-convolutional-neural-networks-71ef1190522d}
			\end{itemize}
		\end{itemize}
	


	% -------------------- %
	% Declaration %
	% -------------------- %
	\pagebreak
	\section*{Declaration}
		\thispagestyle{empty}
		
		\noindent I hereby declare that the work presented in this thesis is solely my work and that to the best of my
		knowledge this work is original, except where indicated by references to other authors. No part of this
		work has been submitted for any other degree or diploma. 
		
		\begin{displaymath}
		% use packages: array
		\begin{array}{ll}
		Signature:~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		& Place, Date:~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		\end{array}
		\end{displaymath}

\end{document}