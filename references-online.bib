
@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Connectors}},
	url = {https://www.zotero.org/download/connectors},
	urldate = {2020-02-25},
	file = {Zotero | Connectors:C\:\\Users\\bjoern\\Zotero\\storage\\RTTUKGHL\\connectors.html:text/html}
}

@misc{noauthor_most_2015,
	type = {{CT904}},
	title = {The {Most} {Popular} {Language} {For} {Machine} {Learning} {Is} ... ({IT} {Best} {Kept} {Secret} {Is} {Optimization})},
	copyright = {© Copyright IBM Corporation 2009, 2013},
	url = {www.ibm.com/developerworks/community/blogs/jfp/entry/what_language_is_best_for_machine_learning_and_data_science},
	abstract = {developerWorks wikis allow groups of people to jointly create and maintain content through contribution and collaboration. Wikis apply the wisdom of crowds to generating information for users interested in a particular subject. You can search all wikis, start a wiki,and view the wikis you own, the wikis you interact with as an editor or reader, and the wikis you follow.},
	language = {en},
	urldate = {2020-02-25},
	month = aug,
	year = {2015},
	note = {Library Catalog: www.ibm.com},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\USFZ8UXW\\What_Language_Is_Best_For_Machine_Learning_And_Data_Science.html:text/html}
}

@misc{sagar_deep_2019,
	title = {Deep {Learning} for {Image} {Classification} with {Less} {Data}},
	url = {https://towardsdatascience.com/deep-learning-for-image-classification-with-less-data-90e5df0a7b8e},
	abstract = {Deep Learning is indeed possible with less data},
	language = {en},
	urldate = {2020-02-25},
	journal = {Medium},
	author = {Sagar, Abhinav},
	month = nov,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\9UJP8ULV\\deep-learning-for-image-classification-with-less-data-90e5df0a7b8e.html:text/html}
}

@misc{warden_how_2017,
	title = {How many images do you need to train a neural network?},
	url = {https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/},
	abstract = {Photo by Glenn Scott Today I got an email with a question I’ve heard many times – “How many images do I need to train my classifier?”. In the early days I would reply with t…},
	language = {en},
	urldate = {2020-02-25},
	journal = {Pete Warden's blog},
	author = {Warden, Pete},
	month = dec,
	year = {2017},
	note = {Library Catalog: petewarden.com},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\YULDLUMR\\how-many-images-do-you-need-to-train-a-neural-network.html:text/html}
}

@misc{noauthor_what_nodate,
	title = {What is the minimum sample size required to train a {Deep} {Learning} model - {CNN}?},
	url = {https://www.researchgate.net/post/What_is_the_minimum_sample_size_required_to_train_a_Deep_Learning_model-CNN},
	abstract = {Read 23 answers by scientists with 79 recommendations from their colleagues to the question asked by Ebenezer R.H.P. Isaac on Feb 8, 2016},
	language = {en},
	urldate = {2020-02-25},
	journal = {ResearchGate},
	note = {Library Catalog: www.researchgate.net},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\HAX7CQWE\\What_is_the_minimum_sample_size_required_to_train_a_Deep_Learning_model-CNN.html:text/html}
}

@misc{noauthor_convolutional_2020,
	title = {Convolutional neural network},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&oldid=942501792},
	abstract = {In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.},
	language = {en},
	urldate = {2020-02-25},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 942501792},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\ZRNZFDKF\\index.html:text/html}
}

@article{szeliski_computer_nodate,
	title = {Computer {Vision}: {Algorithms} and {Applications}},
	language = {en},
	author = {Szeliski, Richard},
	pages = {979},
	file = {Szeliski - Computer Vision Algorithms and Applications.pdf:C\:\\Users\\bjoern\\Zotero\\storage\\43KYEMGE\\Szeliski - Computer Vision Algorithms and Applications.pdf:application/pdf}
}

@misc{noauthor_vapnikchervonenkis_2020,
	title = {Vapnik–{Chervonenkis} dimension},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Vapnik%E2%80%93Chervonenkis_dimension&oldid=942482212},
	abstract = {In Vapnik–Chervonenkis theory, the Vapnik–Chervonenkis (VC) dimension is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a space of functions that can be learned by a statistical classification algorithm. It is defined as the cardinality of the largest set of points that the algorithm can shatter. It was originally defined by Vladimir Vapnik and Alexey Chervonenkis.Informally, the capacity of a classification model is related to how complicated it can be. For example, consider the thresholding of a high-degree polynomial: if the polynomial evaluates above zero, that point is classified as positive, otherwise as negative. A high-degree polynomial can be wiggly, so it can fit a given set of training points well. But one can expect that the classifier will make errors on other points, because it is too wiggly. Such a polynomial has a high capacity. A much simpler alternative is to threshold a linear function. This function may not fit the training set well, because it has a low capacity. This notion of capacity is made rigorous below.},
	language = {en},
	urldate = {2020-02-25},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 942482212},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\RNYYQRHR\\index.html:text/html}
}

@misc{noauthor_overfitting_2020,
	title = {Overfitting},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Overfitting&oldid=942053730},
	abstract = {In statistics, overfitting is "the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably". An overfitted model is a statistical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure.Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data. An underfitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Underfitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
Overfitting and underfitting can occur in machine learning, in particular. In machine learning, the phenomena are sometimes called "overtraining" and "undertraining". 
The possibility of overfitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then overfitting occurs when a model begins to "memorize" training data rather than "learning" to generalize from a trend. 
As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. 
The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.
To lessen the chance of, or amount of, overfitting, several techniques are available (e.g. model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.},
	language = {en},
	urldate = {2020-02-25},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 942053730},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\LIR4PH8P\\index.html:text/html}
}

@misc{noauthor_perceptron_2020,
	title = {Perceptron},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Perceptron&oldid=942271496},
	abstract = {In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.},
	language = {en},
	urldate = {2020-02-25},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 942271496},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\F9CJN728\\index.html:text/html}
}

@misc{noauthor_least_2020,
	title = {Least mean squares filter},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Least_mean_squares_filter&oldid=941899198},
	abstract = {Least mean squares (LMS) algorithms are a class of adaptive filter used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired and the actual signal). It is a stochastic gradient descent method in that the filter is only adapted based on the error at the current time.  It was invented in 1960 by Stanford University professor Bernard Widrow and his first Ph.D. student, Ted Hoff.},
	language = {en},
	urldate = {2020-02-25},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 941899198},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\4MDVNMAZ\\index.html:text/html}
}

@misc{noauthor_backpropagation_2020,
	title = {Backpropagation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Backpropagation&oldid=939314095},
	abstract = {In machine learning, specifically deep learning, backpropagation (backprop, BP) is a widely used algorithm in training feedforward neural networks for supervised learning. Generalizations of backpropagation exist for other artificial neural networks (ANNs), and for functions generally – a class of algorithms is referred to generically as "backpropagation". In deep learning, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input–output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming.The term backpropagation strictly refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent. Backpropagation generalizes the gradient computation in the Delta rule, which is the single-layer version of backpropagation, and is in turn generalized by automatic differentiation, where backpropagation is a special case of reverse accumulation (or "reverse mode"). The term backpropagation and its general use in neural networks was announced in Rumelhart, Hinton \& Williams (1986a), then elaborated and popularized in Rumelhart, Hinton \& Williams (1986b), but the technique was independently rediscovered many times, and had many predecessors dating to the 1960s; see § History. A modern overview is given in Goodfellow, Bengio \& Courville (2016).},
	language = {en},
	urldate = {2020-02-25},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 939314095},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\KNCN5VTB\\index.html:text/html}
}

@misc{noauthor_confusion_2020,
	title = {Confusion matrix},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Confusion_matrix&oldid=940280604},
	abstract = {In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).
It is a special kind of contingency table, with two dimensions ("actual" and "predicted"), and identical sets of "classes" in both dimensions (each combination of dimension and class is a variable in the contingency table).},
	language = {en},
	urldate = {2020-02-25},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 940280604},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\ERMSAH8U\\index.html:text/html}
}

@misc{noauthor_verlustfunktion_2018,
	title = {Verlustfunktion ({Statistik})},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://de.wikipedia.org/w/index.php?title=Verlustfunktion_(Statistik)&oldid=177095272},
	abstract = {Eine Verlustfunktion ist eine spezielle Funktion in der mathematischen Statistik und Teil eines statistischen Entscheidungsproblemes. Sie ordnet jeder Entscheidung in Form einer Punktschätzung, einer Bereichsschätzung oder eines Tests den Schaden zu, der durch eine vom wahren Parameter abweichende Entscheidung entsteht. Gemeinsam mit der Entscheidungsfunktion wird die Verlustfunktion zur Risikofunktion kombiniert, die den potentiellen Schaden bei Verwendung einer Entscheidungsfunktion angibt.},
	language = {de},
	urldate = {2020-02-26},
	journal = {Wikipedia},
	month = may,
	year = {2018},
	note = {Page Version ID: 177095272},
	file = {Snapshot:C\:\\Users\\bjoern\\Zotero\\storage\\NYCJQ5AS\\index.html:text/html}
}